{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages & Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference : https://gist.github.com/mertedali/ab7078b9c29dea18c72525239d636b96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list=ls())\n",
    "\n",
    "library(data.table)\n",
    "library(tidyverse)\n",
    "library(rJava)\n",
    "library(RNetLogo)\n",
    "\n",
    "library(lhs) # For maximin Latin hypercube sampling\n",
    "library(ggplot2)\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "library(factoextra)\n",
    "library(e1071)\n",
    "library(TSrepr) # for evaluating predictive power\n",
    "\n",
    "require(gridExtra)\n",
    "\n",
    "options(warn = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select if data generation is wanted\n",
    "GenerateTTData <- 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Is_Headless <- 1\n",
    "nl.model <- \"Segregation\"\n",
    "\n",
    "nl.path <- \"C:/Program Files/NetLogo 6.0.4/app\"\n",
    "model.path <- paste0(\"C:/Users/paslanpatir/Desktop/TEZ_v2/\",nl.model,\".nlogo\")\n",
    "\n",
    "if (Is_Headless == 0){\n",
    "    NLStart(nl.path, gui = TRUE,nl.jarname='netlogo-6.0.4.jar')\n",
    "    NLLoadModel (model.path)\n",
    "    } else {\n",
    "    NLStart(nl.path, gui = FALSE,nl.jarname='netlogo-6.0.4.jar')\n",
    "    NLLoadModel (model.path)\n",
    "    \n",
    "    #NLStart(nl.path, gui = FALSE,nl.jarname='netlogo-6.0.4.jar', nl.obj = nl.model)\n",
    "    #NLLoadModel (model.path, nl.obj = nl.model )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "\n",
    "## Set model parameters\n",
    " # Number of replications for each instance\n",
    "nofrep = 1    \n",
    "\n",
    "# Number of iterations\n",
    "numiter = 10\n",
    " # order feature names according to their definition order in run_model\n",
    "feature_names = c(\"density\",\"%-similar-wanted\")  \n",
    " # \n",
    "output_name = c(\"percent-similar\")\n",
    "\n",
    " # Number of input parameters of the agent-based model\n",
    "nofparams = length(feature_names)      \n",
    "\n",
    "# set RF parameters\n",
    "ntree = 400\n",
    "mtry = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_type = \"RMSE\" # MAPE, BIAS\n",
    "\n",
    "# choose the uncertainty measure\n",
    "selection_metric <- \"sd\" #, \"range\" \n",
    "\n",
    "unlabeled_ins = 700 \n",
    "test_ins = 400\n",
    "train_ins_oneshot = 700\n",
    "train_ins_Ad = 200\n",
    "\n",
    "# Set selection parameters\n",
    "selected_ins = 5 #nofinstancesWillbeSelected in each step\n",
    "\n",
    "# Set elimination parameters\n",
    "h <- 1 # number of variables eliminated in each step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_model <- function(feature_names,feature_values){ # both should be in character list format\n",
    "run_model <- function(feature_values){ # both should be in character list format\n",
    "\n",
    "    \n",
    "    k = length(feature_names)    \n",
    "    for(i in 1:k){\n",
    "        NLCommand(paste0(\"set \",feature_names[i],\" \",feature_values[i]))      \n",
    "    }\n",
    "    NLCommand(\"setup\")\n",
    "    NLDoCommand(100, \"go\") \n",
    "    result <- NLReport(output_name)\n",
    "    return(result)   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_replicas <- function(nofrep,feature_names,feature_values) {\n",
    "run_replicas <- function(nofrep,feature_values) {\n",
    "    replicas = matrix(NA, ncol = nofrep, nrow = 1) # Save the result of each replication\n",
    "    for(i in 1:nofrep){\n",
    "     #   replicas[i]= run_model(feature_names,feature_values)\n",
    "        replicas[i]= run_model(feature_values)\n",
    "    }\n",
    "    aggregated_result = mean(replicas)\n",
    "    return(aggregated_result)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_ABM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_ABM = function(nofrep,nofinstances,unlabeledset,featurenames = feature_names){\n",
    "run_ABM = function(nofrep,nofinstances,unlabeledset){\n",
    "   #unlabeledset = setcolorder(unlabeledset,featurenames) \n",
    "   unlabeledset = setcolorder(unlabeledset,feature_names) \n",
    "   for(i in 1:nofinstances){\n",
    "        #unlabeledset[i, output :=  run_replicas(nofrep,featurenames, as.matrix(unlabeledset[i,]))]    \n",
    "        unlabeledset[i, output :=  run_replicas(nofrep, as.matrix(unlabeledset[i,]))] \n",
    "    } \n",
    "    return(unlabeledset)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error functions on test data\n",
    "rmse_func <- function(actual, predicted){\n",
    "    error = predicted - actual\n",
    "    return(sqrt(mean(error^2)))\n",
    "}\n",
    "\n",
    "mape_func <- function(actual,predicted){\n",
    "    return( (abs(actual - predicted)/ actual)*100 )\n",
    "}\n",
    "\n",
    "bias_func <- function(actual,predicted){\n",
    "    return( (actual - predicted)/ actual )\n",
    "}\n",
    "\n",
    "#error functions on train data\n",
    "obb_error_func <- function(model){\n",
    "   if(model$type == \"regression\"){\n",
    "        oob_error = model$mse[model$ntree] \n",
    "    }else if(model$type == \"classification\"){\n",
    "        oob_error = model$err.rate \n",
    "    } \n",
    "    return(oob_error)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction functions\n",
    "get_test_predictions <- function(model,testset,errortype){\n",
    "    \n",
    "    predictedLabels <- predict(model, testset)\n",
    "    predictedLabels <- cbind(testset,predictedLabels)\n",
    "    setnames(predictedLabels, \"predictedLabels\",\"pred_output\")\n",
    "\n",
    "    output_variables = colnames(select(predictedLabels, contains(\"output\")))\n",
    "    # output_variables[1] = true output\n",
    "    # output_variables[2] = predicted output\n",
    "    \n",
    "    #output_variables = colnames(predictedLabels[,1:(ncol(predictedLabels) - 2)])\n",
    "    \n",
    "    if(error_type == \"MAPE\"){\n",
    "        predictedLabels[,MAPE := mapply(function(x,y) mape_func(x,y),get(output_variables[1]),get(output_variables[2]))]\n",
    "          }\n",
    "    if(error_type == \"RMSE\"){\n",
    "        predictedLabels[,RMSE := mapply(function(x,y) rmse_func(x,y),get(output_variables[1]),get(output_variables[2]))]\n",
    "          }\n",
    "    if(error_type == \"BIAS\"){\n",
    "        predictedLabels[,BIAS := mapply(function(x,y) bias_func(x,y),get(output_variables[1]),get(output_variables[2]))]\n",
    "           } \n",
    "                                  \n",
    "     output_variables_1 = predictedLabels[,get(output_variables[1]), with = TRUE]\n",
    "     output_variables_2 = predictedLabels[,get(output_variables[2]), with = TRUE]\n",
    "    \n",
    "     performance_temp = matrix(c(1:3), nrow = 1, ncol = 3)\n",
    "     performance_temp[1] =  mae(output_variables_1 , output_variables_2)\n",
    "     performance_temp[2] = rmse(output_variables_1 , output_variables_2)\n",
    "     performance_temp[3] = mape(output_variables_1 , output_variables_2)\n",
    "    \n",
    "    return(list(predictedLabels,performance_temp,output_variables))\n",
    "    \n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive sample selection function with an uncertainty measure depending on \"selection_metric\"\n",
    "sample_selection <- function(selected_ins,unlabeled_set,model){\n",
    "    \n",
    "    ind_pred <- t(predict(model, unlabeled_set,predict.all = TRUE)$individual) %>% data.table() # predictions by each tree in the forest\n",
    "    ind_pred_eval = data.table()\n",
    "    \n",
    "    # standard deviation calculation\n",
    "    s_dev = sapply(ind_pred, sd) %>% data.table()\n",
    "    setnames(s_dev,\".\",\"sd\")\n",
    "    ind_pred_eval = cbind(ind_pred_eval,s_dev)\n",
    "    \n",
    "    # range calculation\n",
    "    range = sapply(ind_pred, range) %>% t() %>% data.table()\n",
    "    range = range[,.(range = abs(range[,1] - range[,2]))]\n",
    "    setnames(range,\"range.V1\",\"range\")\n",
    "    ind_pred_eval = cbind(ind_pred_eval,range)\n",
    "    \n",
    "    ind_pred_eval[,idx := 1:.N]\n",
    "    \n",
    "    if(selection_metric == \"sd\") {\n",
    "      ind_pred_eval = ind_pred_eval[order(-sd)][1:selected_ins]\n",
    "    }else if(selection_metric == \"range\"){\n",
    "      ind_pred_eval = ind_pred_eval[order(-range)][1:selected_ins]\n",
    "    }\n",
    "    \n",
    "    unlabeled_set[,idx := 1:.N]    \n",
    "    train_candidates = unlabeled_set[ind_pred_eval$idx]\n",
    "    \n",
    "    return(train_candidates)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random_sample_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample selection\n",
    "random_sample_selection <- function(selected_ins,unlabeled_set){\n",
    "  \n",
    "    unlabeled_set[,idx := 1:.N]\n",
    "    \n",
    "    train_candidate_idx = sample(unlabeled_set$idx, selected_ins, replace = FALSE, prob = NULL)   \n",
    "    train_candidates = unlabeled_set[idx %in% train_candidate_idx]\n",
    "    \n",
    "    return(train_candidates)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_variable_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_variable_importance <- function(model){\n",
    "    importances <- importance(model, type = 1, scale = FALSE)\n",
    "    selected.vars <- order(importances, decreasing = TRUE)\n",
    "    ranked_features = feature_names[selected.vars]\n",
    "    ordered.importances <- importances[selected.vars]\n",
    "    \n",
    "    return(ranked_features)\n",
    "}                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_elimination <- function(h,total_numof_eliminated_vars,ranked_features){ \n",
    "    numof_columns_left = length(ranked_features) - (total_numof_eliminated_vars + h)\n",
    "    columns_left = ranked_features[1:numof_columns_left]\n",
    "    \n",
    "    eliminated_columns = setdiff((length(ranked_features) - total_numof_eliminated_vars), numof_columns_left)\n",
    "    eliminated_columns = ranked_features[eliminated_columns]\n",
    "    \n",
    "    # update total_numof_eliminated_vars\n",
    "    total_numof_eliminated_vars = length(ranked_features) - length(columns_left)\n",
    "    \n",
    "    return(list(columns_left,total_numof_eliminated_vars,h,eliminated_columns))\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Unlabeled Data Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latin hyper cube sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(GenerateTTData == 1){\n",
    "    unlabeled_pool = as.data.table(maximinLHS(n = unlabeled_ins, k = nofparams, dup = 5))\n",
    "    \n",
    "    unlabeled_pool$V1 = qunif(unlabeled_pool$V1, 10, 90) \n",
    "    unlabeled_pool$V2 = qunif(unlabeled_pool$V2, 10, 90) \n",
    "    setnames(unlabeled_pool, c(paste0(\"V\",1:nofparams)), feature_names)\n",
    "    \n",
    "    unlabeled_pool[,idx := 1:.N]\n",
    "        \n",
    "    fwrite(unlabeled_pool, paste0(\"C:/Users/paslanpatir/Desktop/TEZ_v2/unlabeled_pool_\",Sys.Date(),\".csv\"))\n",
    "}else{\n",
    "    unlabeled_pool <- fread(\"C:/Users/paslanpatir/Desktop/TEZ_v2/unlabeled_pool_04122019.csv\")   \n",
    "    unlabeled_pool <- head(unlabeled_pool[`%-similar-wanted` < 90 & `density` < 90],700) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_unlabeled_pool <- princomp(unlabeled_pool[,-c(\"idx\")], cor = TRUE, scores = TRUE)\n",
    "pca_unlabeled_pool_components <- get_pca_ind(pca_unlabeled_pool)\n",
    "p_unlabeled_pool <- ggplot(data = data.table(pca_unlabeled_pool_components$coord[,1:2]), aes(x = Dim.1, y = Dim.2)) +\n",
    "                    geom_point() +\n",
    "                    labs( title = \"\") \n",
    "p_unlabeled_pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(GenerateTTData == 1){\n",
    "    test_set <- head(unlabeled_pool,test_ins)\n",
    "    \n",
    "    ################## Buraya variale'ların datatipine göre bir şeyler yazılabilir\n",
    "    test_set$density            = runif(test_ins, 10, 90) \n",
    "    test_set$`%-similar-wanted` = runif(test_ins, 10, 90) \n",
    "    test_set[,c(\"idx\"):= NULL]\n",
    "       \n",
    "    print(paste0(\"ABM run start time : \",Sys.time()))\n",
    "    test_set = run_ABM(nofrep,test_ins,test_set) %>% as.data.table()\n",
    "    print(paste0(\"ABM run end time : \",Sys.time()))\n",
    "    \n",
    "    fwrite(test_set, paste0(\"C:/Users/paslanpatir/Desktop/TEZ_v2/test_set_\",Sys.Date(),\".csv\"))\n",
    "}else{\n",
    "    test_set <- fread(\"C:/Users/paslanpatir/Desktop/TEZ_v2/test_set_04122019.csv\")  \n",
    "    test_set <- head(test_set[`%-similar-wanted` < 90],800) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 * 10 ~ 1 min\n",
    "100 * 10 ~ 14 min\n",
    "900 * 10 ~ 09:16 -- 2019-12-03 07:54:10 +03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test_set <- princomp(test_set, cor = TRUE, scores = TRUE)\n",
    "pca_test_set_components <- get_pca_ind(pca_test_set)\n",
    "p_test_set <- ggplot(data = data.table(pca_test_set_components$coord[,1:2]), aes(x = Dim.1, y = Dim.2)) +\n",
    "                    geom_point() +\n",
    "                    labs( title = \"\") \n",
    "p_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark : One-shot sampling, No feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a very big data pool ( nofinstances should be very high ) , like 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(GenerateTTData == 1){\n",
    "    training_set = as.data.table(maximinLHS(n = train_ins_oneshot, k = nofparams, dup = 5))\n",
    "    \n",
    "    training_set$V1 = qunif(training_set$V1, 10, 90) \n",
    "    training_set$V2 = qunif(training_set$V2, 10, 90) \n",
    "    setnames(training_set, c(paste0(\"V\",1:nofparams)), feature_names)\n",
    "    \n",
    "    training_set$output <- 0.00\n",
    "    \n",
    "    print(paste0(\"ABM run start time : \",Sys.time()))\n",
    "    training_set = run_ABM(nofrep,train_ins_oneshot,LHSample) %>% as.data.table()\n",
    "    print(paste0(\"ABM run end time : \",Sys.time()))  \n",
    "    \n",
    "    fwrite(training_set, paste0(\"C:/Users/paslanpatir/Desktop/TEZ_v2/training_set_\",Sys.Date(),\".csv\"))\n",
    "    \n",
    "}else{\n",
    "    training_set <- fread(\"C:/Users/paslanpatir/Desktop/TEZ_v2/LHSample_Data_04122019.csv\")\n",
    "    training_set <- head(training_set[`%-similar-wanted` < 90],700)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_data = copy(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_training_set_components <- get_pca_ind(pca_training_set)\n",
    "pca_training_set_components <-cbind(pca_training_set_components$coord[,1:2],training_set[,.SD, .SDcols = c(\"output\")])\n",
    "p_training_set <- ggplot(data = pca_training_set_components, aes(x = Dim.1, y = Dim.2)) +\n",
    "             geom_point(aes(colour = output)) +\n",
    "             labs( title = \"\", legend = \"output\") \n",
    "p_training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_oneshot <- randomForest(x = training_set[, -c(\"output\")], y = training_set$output, importance = TRUE,ntree = ntree, mtry = mtry)\n",
    "model_oneshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obb_error_oneshot <- obb_error_func(model_oneshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBB_pred = cbind(training_set$output,model_oneshot$predicted)\n",
    "#names(OBB_pred) <- c(\"actual\",\"predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(model_oneshot$mse, type=\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction_oneshot = get_test_predictions(model_oneshot,test_set,error_type)\n",
    "predictedLabels_oneshot = test_prediction_oneshot[[1]]\n",
    "\n",
    "performance_table_oneshot = data.table(iter = numeric(), mae= numeric(),rmse= numeric(), mape = numeric())\n",
    "# Keep test set error records\n",
    "performance_table_oneshot = rbind(performance_table_oneshot, data.table(1, test_prediction_oneshot[[2]]), use.names = FALSE)\n",
    "\n",
    "output_variables = test_prediction_oneshot[[3]]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_oneshot <- ggplot(predictedLabels_oneshot,aes(x = get(output_variables[1]), y = get(output_variables[2]), color = (get(output_variables[2]) - get(output_variables[1])))) +\n",
    "            geom_point() +\n",
    "            geom_abline() +\n",
    "            xlab(\"actual values\") +\n",
    "            ylab(\"fitted values\")\n",
    "\n",
    "p_oneshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling & No Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a relatively big data pool ( nofinstances should be medium) , like 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(GenerateTTData == 1){\n",
    "   \n",
    "    training_set_Ad = as.data.table(maximinLHS(n = train_ins_Ad, k = nofparams, dup = 5))\n",
    "    \n",
    "   training_set_Ad$V1 = qunif(training_set_Ad$V1, 10, 90) \n",
    "   training_set_Ad$V2 = qunif(training_set_Ad$V2, 10, 90) \n",
    "    setnames(training_set_Ad, c(\"V1\",\"V2\"), feature_names)\n",
    "    training_set_Ad$output <- 0.00\n",
    "    \n",
    "    print(paste0(\"ABM run start time : \",Sys.time()))\n",
    "    training_set_Ad = run_ABM(nofrep,train_ins_Ad,training_set_Ad) %>% as.data.table()\n",
    "    print(paste0(\"ABM run end time : \",Sys.time()))\n",
    "    \n",
    "    fwrite(training_set_Ad, paste0(\"C:/Users/paslanpatir/Desktop/TEZ_v2/LHSample_Ad_Data\",Sys.Date(),\".csv\"))\n",
    "\n",
    "}else{\n",
    "    training_set_Ad <- fread(\"C:/Users/paslanpatir/Desktop/TEZ_v2/LHSample_Ad_Data_04122019.csv\")\n",
    "    training_set_Ad <- head(training_set_Ad[`%-similar-wanted` < 90  & `density` < 90],200)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_initial_data = copy(training_set_Ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on strategy:\n",
    "iteration_budget = 3\n",
    "\n",
    "## initialize record tables Record train candidates\n",
    "train_candidates_table = data.table()\n",
    "\n",
    "# Record model performances\n",
    "performance_table = data.table(iter = numeric(), mae = numeric(), rmse = numeric(), mape = numeric())\n",
    "\n",
    "# Record obb_error table\n",
    "obb_error = data.table(iter = numeric(), obb_error = numeric())\n",
    "\n",
    "## initialize variables\n",
    "# keep test set undistorted\n",
    "predictedLabels_table = copy(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1\n",
    "while(iter <= iteration_budget){   \n",
    "    print(iter)\n",
    "\n",
    "    trainx = training_set_Ad[,.SD, .SDcols = feature_names]\n",
    "    trainy = training_set_Ad$output\n",
    "    \n",
    "    # Train the model\n",
    "    model_Sub <- randomForest( x = trainx, y =  trainy,importance = TRUE,ntree = ntree, mtry = mtry)\n",
    "    assign(paste0(\"model_Sub_\",iter),model_Sub)\n",
    "                     \n",
    "    obb_error = rbind(obb_error,data.table(iter,obb_error_func(model_Sub)),use.names=FALSE)\n",
    "    \n",
    "    # test the model on test set\n",
    "    test_predictions_Sub = get_test_predictions(model_Sub,test_set,error_type)\n",
    "    predictedLabels_Sub = test_predictions_Sub[[1]]\n",
    "    setnames(predictedLabels_Sub,c(\"pred_output\",error_type), c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter)))    \n",
    "    predictedLabels_table = cbind(predictedLabels_table,predictedLabels_Sub[,.SD, .SDcols = c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter))])\n",
    "    \n",
    "    # Keep test set error records\n",
    "    performance_table = rbind(performance_table,data.table(iter,test_predictions_Sub[[2]]), use.names = FALSE)    \n",
    "\n",
    "    if(iter != iteration_budget){ # below efforts are unnecessary when the budget is reached.\n",
    "        \n",
    "    ## sample selection from unlabeled data select candidates\n",
    "    unlabeled_set <- copy(unlabeled_pool)\n",
    "    train_candidates = random_sample_selection(selected_ins,unlabeled_set)\n",
    "        \n",
    "    # Eliminate train candidates from the unlabeled pool\n",
    "    unlabeled_pool = unlabeled_pool[- train_candidates$idx]\n",
    "    rm(unlabeled_set)\n",
    "    \n",
    "    # run ABM to find outputs of train candidates\n",
    "    print(paste0(\"ABM train_candidate run start time : \",Sys.time()))\n",
    "    train_candidates = run_ABM(nofrep,selected_ins,train_candidates)\n",
    "    print(paste0(\"ABM train_candidate run end time : \",Sys.time()))\n",
    "    \n",
    "    train_candidates_table = rbind(train_candidates_table, data.table(train_candidates,iter = iter))\n",
    "\n",
    "    # Add new data to train data\n",
    "    training_set_Ad = rbind(training_set_Ad,train_candidates[,-c(\"idx\")])\n",
    "    }\n",
    "    iter = iter + 1\n",
    "}\n",
    "\n",
    "# plot koy her iteration'da göstersin.\n",
    "#setcolorder(data,variableorder) ################# bunu bi yerlere koyman gerekebilir, dikkat!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final records\n",
    "FinalTrainData_Rd = copy(training_set_Ad)\n",
    "performance_table_Rd = copy(performance_table)\n",
    "train_candidates_table_Rd  = copy(train_candidates_table)\n",
    "predictedLabels_table_Rd = copy(predictedLabels_table)\n",
    "obb_error_Rd = copy(obb_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "nrow(FinalTrainData_Rd)\n",
    "performance_table_Rd \n",
    "train_candidates_table_Rd  \n",
    "head(predictedLabels_table_Rd)\n",
    "obb_error_Rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_molten_Rd <- melt(data = performance_table_Rd\n",
    "                             , id.vars = 'iter')\n",
    "setnames(performance_molten_Rd, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))\n",
    "p_Rd = ggplot(performance_molten_Rd, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "          geom_line(lwd=1)\n",
    "p_Rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Sampling & No Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a relatively big data pool ( nofinstances should be medium) , like 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_Ad = copy(adaptive_initial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if(GenerateTTData == 1){\n",
    "#   \n",
    "#    LHSample_Ad = as.data.table(maximinLHS(n = train_ins_Ad, k = nofparams, dup = 5))\n",
    "#    \n",
    "#    LHSample_Ad$V1 = qunif(LHSample_Ad$V1, 10, 90) \n",
    "#    LHSample_Ad$V2 = qunif(LHSample_Ad$V2, 10, 90) \n",
    "#    setnames(LHSample_Ad, c(\"V1\",\"V2\"), feature_names)\n",
    "#    LHSample_Ad$output <- 0.00\n",
    "#    \n",
    "#    paste0(\"ABM run start time : \",Sys.time())\n",
    "#    LHSample_Ad = run_ABM(nofrep,train_ins_Ad,LHSample_Ad) %>% as.data.table()\n",
    "#    paste0(\"ABM run end time : \",Sys.time())\n",
    "#    \n",
    "#    fwrite(LHSample_Ad, paste0(\"C:/Users/paslanpatir/Desktop/TEZ_v2/LHSample_Ad_Data\",Sys.Date(),\".csv\"))\n",
    "#\n",
    "#}else{\n",
    "#    LHSample_Ad <- fread(\"C:/Users/paslanpatir/Desktop/TEZ_v2/LHSample_Ad_Data_04122019.csv\")\n",
    "#    LHSample_Ad <- head(LHSample_Ad[`%-similar-wanted` < 90  & `density` < 90],200)\n",
    "#\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_training_set_Ad <- princomp(training_set_Ad[,-c(\"output\")], cor = TRUE, scores = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fviz_pca_ind(pca_LHSample,\n",
    "#             col.ind = \"cos2\", # Color by the quality of representation\n",
    "#             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n",
    "#              geom=\"point\"\n",
    "#             )\n",
    "\n",
    "pca_training_set_Ad_components <- get_pca_ind(pca_training_set_Ad)\n",
    "pca_training_set_Ad_components <-cbind(pca_training_set_Ad_components$coord[,1:2],training_set_Ad[,c(\"output\")])\n",
    "p_training_set_Ad <- ggplot(data = pca_training_set_Ad_components, aes(x = Dim.1, y = Dim.2)) +\n",
    "                     geom_point(aes(colour = output)) +\n",
    "                     labs( title = \"\", legend = \"output\") \n",
    "p_training_set_Ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on strategy:\n",
    "iteration_budget = 3\n",
    "\n",
    "h = 1 # specify how many variable will be eliminated in each elimination iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize record tables Record train candidates\n",
    "train_candidates_table = data.table()\n",
    "\n",
    "# Record model performances\n",
    "performance_table = data.table(iter = numeric(), mae = numeric(), rmse = numeric(), mape = numeric())\n",
    "\n",
    "# Record obb_error table\n",
    "obb_error = data.table(iter = numeric(), obb_error = numeric())\n",
    "\n",
    "## initialize variables\n",
    "# keep test set undistorted\n",
    "predictedLabels_table = copy(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Sys.time())\n",
    "iter = 1\n",
    "while(iter <= iteration_budget){   \n",
    "    print(iter)\n",
    "\n",
    "    trainx = training_set_Ad[,.SD, .SDcols = feature_names]\n",
    "    trainy = training_set_Ad$output\n",
    "    \n",
    "    # Train the model\n",
    "    model_Sub <- randomForest( x = trainx, y =  trainy,importance = TRUE,ntree = ntree, mtry = mtry)\n",
    "    assign(paste0(\"model_Sub_\",iter),model_Sub)\n",
    "                    \n",
    "    obb_error = rbind(obb_error,data.table(iter,obb_error_func(model_Sub)),use.names=FALSE)\n",
    "\n",
    "    # test the model on test set\n",
    "    test_predictions_Sub = get_test_predictions(model_Sub,test_set,error_type)\n",
    "    predictedLabels_Sub = test_predictions_Sub[[1]]\n",
    "    setnames(predictedLabels_Sub,c(\"pred_output\",error_type), c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter)))    \n",
    "    predictedLabels_table = cbind(predictedLabels_table,predictedLabels_Sub[,.SD, .SDcols = c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter))])\n",
    "    \n",
    "    # Keep test set error records\n",
    "    performance_table = rbind(performance_table,data.table(iter,test_predictions_Sub[[2]]), use.names = FALSE)\n",
    "    \n",
    "    if(iter != iteration_budget){ # below efforts are unnecessary when the budget is reached.    \n",
    "    ## sample selection from unlabeled data select candidates\n",
    "        unlabeled_set <- copy(unlabeled_pool)\n",
    "        train_candidates = sample_selection(selected_ins, unlabeled_set, model_Sub)\n",
    "        \n",
    "        # eliminate candidates from the unlabeled pool\n",
    "        unlabeled_pool = unlabeled_pool[-train_candidates$idx]\n",
    "        rm(unlabeled_set)\n",
    "        \n",
    "        # run ABM to find outputs of train candidates\n",
    "        paste0(\"ABM train_candidate run start time : \", Sys.time())\n",
    "        train_candidates = run_ABM(nofrep, selected_ins, train_candidates)\n",
    "        paste0(\"ABM train_candidate run end time : \", Sys.time())\n",
    "        \n",
    "        train_candidates_table = rbind(train_candidates_table, data.table(train_candidates,iter = iter))\n",
    "        \n",
    "        # add labeled candidates to the train data\n",
    "        training_set_Ad = rbind(training_set_Ad, train_candidates[, -c(\"idx\")])\n",
    "    }\n",
    "    iter = iter + 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final records\n",
    "FinalTrainData_Ad = copy(training_set_Ad)\n",
    "performance_table_Ad = copy(performance_table)\n",
    "train_candidates_table_Ad  = copy(train_candidates_table)\n",
    "predictedLabels_table_Ad = copy(predictedLabels_table)\n",
    "obb_error_Ad = copy(obb_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(FinalTrainData_Ad)\n",
    "performance_table_Ad\n",
    "train_candidates_table_Ad\n",
    "head(predictedLabels_table_Ad)\n",
    "obb_error_Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_molten_Ad <- melt(data = performance_table_Ad\n",
    "                             , id.vars = 'iter')\n",
    "setnames(performance_molten_Ad, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))\n",
    "p_Ad = ggplot(performance_molten_Ad, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "            geom_line(lwd=1)\n",
    "p_Ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling vs Uncertainty Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.arrange(p_Rd, p_Ad, ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Sampling & Feature Elimination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a relatively big data pool ( nofinstances should be medium) , like 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_Ad = copy(adaptive_initial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if(GenerateTTData == 1){\n",
    "#   \n",
    "#    LHSample_Ad = as.data.table(maximinLHS(n = train_ins_Ad, k = nofparams, dup = 5))\n",
    "#    \n",
    "#    LHSample_Ad$V1 = qunif(LHSample_Ad$V1, 10, 90) \n",
    "#    LHSample_Ad$V2 = qunif(LHSample_Ad$V2, 10, 90) \n",
    "#    setnames(LHSample_Ad, c(\"V1\",\"V2\"), feature_names)\n",
    "#    LHSample_Ad$output <- 0.00\n",
    "#    \n",
    "#    paste0(\"ABM run start time : \",Sys.time())\n",
    "#    LHSample_Ad = run_ABM(nofrep,train_ins_Ad,LHSample_Ad) %>% as.data.table()\n",
    "#    paste0(\"ABM run end time : \",Sys.time())\n",
    "#    \n",
    "#    fwrite(LHSample_Ad, paste0(\"C:/Users/paslanpatir/Desktop/TEZ_v2/LHSample_Ad_Data\",Sys.Date(),\".csv\"))\n",
    "#\n",
    "#}else{\n",
    "#    LHSample_Ad <- fread(\"C:/Users/paslanpatir/Desktop/TEZ_v2/LHSample_Ad_Data_04122019.csv\")\n",
    "#    LHSample_Ad <- head(LHSample_Ad[`%-similar-wanted` < 90  & `density` < 90],200)\n",
    "#\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_training_set_Ad <- princomp(training_set_Ad[,-c(\"output\")], cor = TRUE, scores = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fviz_pca_ind(pca_LHSample,\n",
    "#             col.ind = \"cos2\", # Color by the quality of representation\n",
    "#             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n",
    "#              geom=\"point\"\n",
    "#             )\n",
    "\n",
    "pca_training_set_Ad_components <- get_pca_ind(pca_training_set_Ad)\n",
    "pca_training_set_Ad_components <-cbind(pca_training_set_Ad_components$coord[,1:2],training_set_Ad[,c(\"output\")])\n",
    "p_training_set_Ad <- ggplot(data = pca_training_set_Ad_components, aes(x = Dim.1, y = Dim.2)) +\n",
    "                     geom_point(aes(colour = output)) +\n",
    "                     labs( title = \"\", legend = \"output\") \n",
    "p_training_set_Ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on strategy:\n",
    "sample_selection_iteration_order = c(1,3)\n",
    "feature_elimination_iteration_order = c(2,3)\n",
    "iteration_budget = 3 # should be > max(max(sample_selection_iteration_order),max(feature_elimination_iteration_order))\n",
    "\n",
    "h = 1 # specify how many variable will be eliminated in each elimination iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize record tables Record train candidates\n",
    "train_candidates_table = data.table()\n",
    "\n",
    "# Record model performances\n",
    "performance_table = data.table(iter = numeric(), mae = numeric(), rmse = numeric(), mape = numeric())\n",
    "\n",
    "# Record obb_error table\n",
    "obb_error = data.table(iter = numeric(), obb_error = numeric())\n",
    "\n",
    "# Record iteration history\n",
    "iteration_history = data.table(iter_no = numeric(), IsFeatureEliminated = logical(), IsDataSelected = logical())\n",
    "\n",
    "## initialize variables\n",
    "# keep test set undistorted\n",
    "predictedLabels_table = copy(test_set)\n",
    "\n",
    "# specify variables(columns) to be used initialize\n",
    "columns_left = feature_names\n",
    "total_numof_eliminated_vars <- 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1\n",
    "while (iter <= iteration_budget) {\n",
    "    \n",
    "    trainx = training_set_Ad[, .SD, .SDcols = columns_left]\n",
    "    trainy = training_set_Ad$output\n",
    "    \n",
    "    # Train the model\n",
    "    model_Sub <- randomForest(x = trainx, y = trainy, importance = TRUE, ntree = ntree, mtry = mtry)\n",
    "    assign(paste0(\"model_Sub_\", iter), model_Sub)\n",
    "    \n",
    "    if (length(columns_left) == length(feature_names)) {\n",
    "        ranked_features = get_variable_importance(model_Sub)\n",
    "    }\n",
    "    # Keep training set error records\n",
    "    obb_error = rbind(obb_error, data.table(iter, obb_error_func(model_Sub)), use.names = FALSE)\n",
    "    \n",
    "    # Test the model on test set\n",
    "    test_predictions_Sub = get_test_predictions(model_Sub, test_set, error_type)\n",
    "    predictedLabels_Sub = test_predictions_Sub[[1]]\n",
    "    setnames(predictedLabels_Sub, c(\"pred_output\", error_type), c(paste0(\"pred_output_\", iter), paste0(error_type, \"_\", iter)))\n",
    "    predictedLabels_table = cbind(predictedLabels_table, predictedLabels_Sub[,.SD, .SDcols = c(paste0(\"pred_output_\", iter), paste0(error_type, \"_\", iter))])\n",
    "    \n",
    "    # Keep test set error records\n",
    "    performance_table = rbind(performance_table, data.table(iter, test_predictions_Sub[[2]]), use.names = FALSE)\n",
    "    \n",
    "    # update iteration_history\n",
    "    iteration_history = rbind(iteration_history, data.table(iter, 0, 0), use.names = FALSE)\n",
    "    \n",
    "    if(iter != iteration_budget){ # below efforts are unnecessary when the budget is reached.\n",
    "          if (iter %in% sample_selection_iteration_order) {\n",
    "              ## sample selection from unlabeled data select candidates\n",
    "              unlabeled_set <- copy(unlabeled_pool)\n",
    "              train_candidates = sample_selection(selected_ins, unlabeled_set, model_Sub)\n",
    "              \n",
    "              # eliminate candidates from the unlabeled pool\n",
    "              unlabeled_pool = unlabeled_pool[-train_candidates$idx]\n",
    "              rm(unlabeled_set)\n",
    "              \n",
    "              # run ABM to find outputs of train candidates\n",
    "              paste0(\"ABM train_candidate run start time : \", Sys.time())\n",
    "              train_candidates = run_ABM(nofrep, selected_ins, train_candidates)\n",
    "              paste0(\"ABM train_candidate run end time : \", Sys.time())\n",
    "              \n",
    "              train_candidates_table = rbind(train_candidates_table, data.table(train_candidates,iter = iter))\n",
    "              \n",
    "              # add labeled candidates to the train data\n",
    "              training_set_Ad = rbind(training_set_Ad, train_candidates[, -c(\"idx\")])\n",
    "              \n",
    "              # update iteration_history\n",
    "               iteration_history[iter]$IsDataSelected= 1\n",
    "          }\n",
    "          if (iter %in% feature_elimination_iteration_order) {\n",
    "              ## feature elimination apply feature elimination\n",
    "              feature_elimination_result = feature_elimination(h, total_numof_eliminated_vars, ranked_features)\n",
    "              \n",
    "              columns_left = feature_elimination_result[[1]]  # \n",
    "              eliminated_columns = feature_elimination_result[[4]]  #   not necessary\n",
    "              total_numof_eliminated_vars = as.numeric(feature_elimination_result[2])\n",
    "              numof_eliminated_vars = as.numeric(feature_elimination_result[3])  #   not necessary \n",
    "              \n",
    "              # update iteration_history\n",
    "              iteration_history[iter]$IsFeatureEliminated= 1\n",
    "          }\n",
    "    }\n",
    "iter = iter + 1  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final records\n",
    "FinalTrainData_AdFe = copy(training_set_Ad)\n",
    "performance_table_AdFe = copy(performance_table)\n",
    "train_candidates_table_AdFe  = copy(train_candidates_table)\n",
    "predictedLabels_table_AdFe = copy(predictedLabels_table)\n",
    "obb_error_AdFe = copy(obb_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(FinalTrainData_AdFe)\n",
    "performance_table_AdFe\n",
    "train_candidates_table_AdFe\n",
    "head(predictedLabels_table_AdFe)\n",
    "obb_error_AdFe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_molten_AdFe <- melt(data = performance_table_AdFe\n",
    "                             , id.vars = 'iter')\n",
    "setnames(performance_molten_AdFe, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))\n",
    "p_AdFe = ggplot(performance_molten_AdFe, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "            geom_line(lwd=1) +\n",
    "            geom_vline(xintercept = iteration_history[IsFeatureEliminated==1]$iter_no + 1, linetype = \"dashed\") +\n",
    "            geom_vline(xintercept = iteration_history[IsDataSelected==1]$iter_no + 1, linetype = \"dotdash\",color = \"yellow\")\n",
    "p_AdFe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#varImpPlot(model_Ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLQuit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 3.6",
   "language": "R",
   "name": "ir36"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "80.9983px",
    "width": "167.995px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "400px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
