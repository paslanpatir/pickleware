{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages & Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "be sure to set:\n",
    "* nl.model\n",
    "* model.type\n",
    "* output_folder\n",
    "* iteration_budget\n",
    "* selected_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list = ls())\n",
    "\n",
    "library(data.table)\n",
    "library(tidyverse)\n",
    "library(rJava)\n",
    "library(RNetLogo)\n",
    "\n",
    "library(lhs)  # For maximin Latin hypercube sampling\n",
    "library(ggplot2)\n",
    "library(plotly)  # For beautiful plotting\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "library(factoextra)\n",
    "library(e1071)\n",
    "library(TSrepr)  # for evaluating predictive power\n",
    "\n",
    "require(gridExtra)\n",
    "\n",
    "options(warn = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Is_Headless <- 1\n",
    "nl.model <- \"Segregation\"\n",
    "\n",
    "nl.path <- \"C:/Program Files/NetLogo 6.0.4/app\"\n",
    "folder.path = \"C:/Users/paslanpatir/Desktop/TEZ_v2/\"\n",
    "model.path <- paste0(folder.path, nl.model, \".nlogo\")\n",
    "\n",
    "if (Is_Headless == 0) {\n",
    "    NLStart(nl.path, gui = TRUE, nl.jarname = \"netlogo-6.0.4.jar\")\n",
    "    NLLoadModel(model.path)\n",
    "} else {\n",
    "    NLStart(nl.path, gui = FALSE, nl.jarname = \"netlogo-6.0.4.jar\", nl.obj = nl.model)\n",
    "    NLLoadModel(model.path, nl.obj = nl.model)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.type = ifelse(nl.model == \"Segregation\", \"basic\", \"dummy\")\n",
    "# the path of data folder\n",
    "data.path = paste0(folder.path,\"data/\")\n",
    "# the path for outputs to be record\n",
    "output.folder = paste0(\"outputs_coefvar_deneme\",Sys.Date())\n",
    "dir.create(file.path(folder.path, output.folder), showWarnings = FALSE)\n",
    "\n",
    "outputs.path = paste0(folder.path,output.folder,\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Me File to keep info about the output folder\n",
    "ReadMe = paste0(outputs.path,\"ReadMe_\",model.type,\".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set model parameters Number of replications for each instance\n",
    "nofrep = 1\n",
    "\n",
    "# order feature names according to their definition order in run_model\n",
    "if (model.type == \"basic\") {\n",
    "    feature_names = c(\"density\", \"%-similar-wanted\")\n",
    "} else if (model.type == \"dummy\") {\n",
    "    feature_names = c(\"density\", \"%-similar-wanted\", \"budget-multiplier-dummy\", \"density-multiplier-dummy\", \n",
    "        \"noise-dummy\", \"tick-limit\")\n",
    "}  \n",
    "# \n",
    "output_name = c(\"percent-similar\")\n",
    "\n",
    "# Number of input parameters of the agent-based model\n",
    "nofparams = length(feature_names)\n",
    "\n",
    "# set RF parameters\n",
    "ntree = 300\n",
    "mtry = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_type = \"RMSE\"  # MAPE, BIAS\n",
    "\n",
    "# choose the uncertainty measure\n",
    "selection_metric <- \"sd\"  #, 'range' \n",
    "\n",
    "# Number of iterations\n",
    "iteration_budget = 11\n",
    "\n",
    "# Number of instances\n",
    "unlabeled_ins = 100\n",
    "test_ins = 100\n",
    "train_ins_oneshot = 100\n",
    "train_ins_Ad = 50\n",
    "\n",
    "# Set selection parameters\n",
    "selected_ins = 25  #nofinstancesWillbeSelected in each step\n",
    "\n",
    "# Set elimination parameters\n",
    "h <- 1  # number of variables eliminated in each step\n",
    "\n",
    "seed.oneshot = 0\n",
    "seed.Ad = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(paste0( \"model =\",nl.model,\"\\n\"\n",
    "             ,\"nofrep =\",nofrep,\"\\n\"\n",
    "             ,\"ntree =\",ntree,\"\\n\"\n",
    "             ,\"mtry =\",mtry,\"\\n\"\n",
    "             ,\"iteration_budget =\",iteration_budget,\"\\n\"\n",
    "             ,\"unlabeled_ins =\",unlabeled_ins,\"\\n\"\n",
    "             ,\"test_ins =\",test_ins,\"\\n\"\n",
    "             ,\"train_ins_Ad =\",train_ins_Ad,\"\\n\"\n",
    "             ,\"selected_ins =\",selected_ins,\"\\n\"\n",
    "             ,\"h =\",h,\"\\n\"\n",
    "             ,paste0(c(\"seed.oneshot =\",seed.oneshot),collapse = \" \"),\"\\n\"\n",
    "             ,paste0(c(\"seed.Ad =\",seed.Ad),collapse = \" \"),\"\\n\"\n",
    "             ,\"error_type =\",error_type,\"\\n\" \n",
    "             ,\"selection_metric =\",selection_metric,\"\\n\" \n",
    "             ,\"Date =\", Sys.Date()\n",
    "             )\n",
    "      ,ReadMe, append=TRUE, sep = \"\\n\" )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_model <- function(feature_names,feature_values){ # both should be in character list format both should be in character list format\n",
    "run_model <- function(feature_values) {\n",
    "    k = length(feature_names)\n",
    "    for (i in 1:k) {\n",
    "        NLCommand(paste0(\"set \", feature_names[i], \" \", feature_values[i]), nl.obj = nl.model)\n",
    "    }\n",
    "    NLCommand(\"setup\", nl.obj = nl.model)\n",
    "    NLDoCommand(100, \"go\", nl.obj = nl.model)\n",
    "    result <- NLReport(output_name, nl.obj = nl.model)\n",
    "    return(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_replicas <- function(nofrep,feature_names,feature_values) {\n",
    "run_replicas <- function(nofrep, feature_values) {\n",
    "    replicas = matrix(NA, ncol = nofrep, nrow = 1)  # Save the result of each replication\n",
    "    for (i in 1:nofrep) {\n",
    "        # replicas[i]= run_model(feature_names,feature_values)\n",
    "        replicas[i] = run_model(feature_values)\n",
    "    }\n",
    "    aggregated_result = mean(replicas)\n",
    "    return(aggregated_result)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_ABM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_ABM = function(nofrep,nofinstances,unlabeledset,featurenames = feature_names){\n",
    "run_ABM = function(nofrep, nofinstances, unlabeledset) {\n",
    "    # unlabeledset = setcolorder(unlabeledset,featurenames)\n",
    "    unlabeledset = setcolorder(unlabeledset, feature_names)\n",
    "    for (i in 1:nofinstances) {\n",
    "        # unlabeledset[i, output := run_replicas(nofrep,featurenames,\n",
    "        # as.matrix(unlabeledset[i,]))]\n",
    "        unlabeledset[i, `:=`(output, run_replicas(nofrep, as.matrix(unlabeledset[i,])))]\n",
    "    }\n",
    "    return(unlabeledset)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error functions on test data\n",
    "rmse_func <- function(actual, predicted) {\n",
    "    error = predicted - actual\n",
    "    return(sqrt(mean(error^2)))\n",
    "}\n",
    "\n",
    "mape_func <- function(actual, predicted) {\n",
    "    return((abs(actual - predicted)/actual) * 100)\n",
    "}\n",
    "\n",
    "bias_func <- function(actual, predicted) {\n",
    "    return((actual - predicted)/actual)\n",
    "}\n",
    "\n",
    "# error functions on train data\n",
    "obb_error_func <- function(model) {\n",
    "    if (model$type == \"regression\") {\n",
    "        oob_error = model$mse[model$ntree]\n",
    "    } else if (model$type == \"classification\") {\n",
    "        oob_error = model$err.rate\n",
    "    }\n",
    "    return(oob_error)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction functions\n",
    "get_test_predictions <- function(model, testset, errortype) {\n",
    "    \n",
    "    predictedLabels <- predict(model, testset)\n",
    "    predictedLabels <- cbind(testset, predictedLabels)\n",
    "    setnames(predictedLabels, \"predictedLabels\", \"pred_output\")\n",
    "    \n",
    "    output_variables = colnames(select(predictedLabels, contains(\"output\")))\n",
    "    # output_variables[1] = true output output_variables[2] = predicted output\n",
    "    \n",
    "    # output_variables = colnames(predictedLabels[,1:(ncol(predictedLabels) - 2)])\n",
    "    \n",
    "    if (error_type == \"MAPE\") {\n",
    "        predictedLabels[, `:=`(MAPE, mapply(function(x, y) mape_func(x, y), get(output_variables[1]), \n",
    "            get(output_variables[2])))]\n",
    "    }\n",
    "    if (error_type == \"RMSE\") {\n",
    "        predictedLabels[, `:=`(RMSE, mapply(function(x, y) rmse_func(x, y), get(output_variables[1]), \n",
    "            get(output_variables[2])))]\n",
    "    }\n",
    "    if (error_type == \"BIAS\") {\n",
    "        predictedLabels[, `:=`(BIAS, mapply(function(x, y) bias_func(x, y), get(output_variables[1]), \n",
    "            get(output_variables[2])))]\n",
    "    }\n",
    "    \n",
    "    output_variables_1 = predictedLabels[, get(output_variables[1]), with = TRUE]\n",
    "    output_variables_2 = predictedLabels[, get(output_variables[2]), with = TRUE]\n",
    "    \n",
    "    performance_temp = matrix(c(1:3), nrow = 1, ncol = 3)\n",
    "    performance_temp[1] = mae(output_variables_1, output_variables_2)\n",
    "    performance_temp[2] = rmse(output_variables_1, output_variables_2)\n",
    "    performance_temp[3] = mape(output_variables_1, output_variables_2)\n",
    "    \n",
    "    return(list(predictedLabels, performance_temp, output_variables))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive sample selection function with an uncertainty measure depending on 'selection_metric'\n",
    "sample_selection <- function(selected_ins, unlabeled_set, model) {\n",
    "    ind_pred <- t(predict(model, unlabeled_set, predict.all = TRUE)$individual) %>% \n",
    "        data.table()  # predictions by each tree in the forest\n",
    "    ind_pred_eval = data.table()\n",
    "    \n",
    "    # standard deviation calculation\n",
    "    s_dev = sapply(ind_pred, sd) %>% data.table()\n",
    "    setnames(s_dev, \".\", \"sd\")\n",
    "    ind_pred_eval = cbind(ind_pred_eval, s_dev)\n",
    "    \n",
    "    # range calculation\n",
    "    range = sapply(ind_pred, range) %>% t() %>% data.table()\n",
    "    range = range[, .(range = abs(range[, 1] - range[, 2]))]\n",
    "    setnames(range, \"range.V1\", \"range\")\n",
    "    ind_pred_eval = cbind(ind_pred_eval, range)\n",
    "    \n",
    "    #coeff variance calculation\n",
    "    s_dev = sapply(ind_pred, sd) %>% data.table()\n",
    "    setnames(s_dev, \".\", \"sd\")\n",
    "    s_mean = sapply(ind_pred, mean) %>% data.table()\n",
    "    setnames(s_mean, \".\", \"mean\")\n",
    "    coeff_var = cbind(s_dev,s_mean) \n",
    "    coeff_var = coeff_var[,.(c_var = (sd / mean)* 100)]\n",
    "    ind_pred_eval = cbind(ind_pred_eval, coeff_var)\n",
    "    \n",
    "    ind_pred_eval[, `:=`(idx, 1:.N)]\n",
    "    \n",
    "    if (selection_metric == \"sd\") {\n",
    "        ind_pred_eval = ind_pred_eval[order(-sd)][1:selected_ins]\n",
    "    } else if (selection_metric == \"range\") {\n",
    "        ind_pred_eval = ind_pred_eval[order(-range)][1:selected_ins]\n",
    "    } else if (selection_metric == \"coefvar\") {\n",
    "        ind_pred_eval = ind_pred_eval[order(-coeff_var)][1:selected_ins]\n",
    "    }\n",
    "    \n",
    "    unlabeled_set[, `:=`(idx, 1:.N)]\n",
    "    train_candidates = unlabeled_set[ind_pred_eval$idx]\n",
    "    \n",
    "    return(train_candidates)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random_sample_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample selection\n",
    "random_sample_selection <- function(selected_ins, unlabeled_set) {\n",
    "    \n",
    "    unlabeled_set[, `:=`(idx, 1:.N)]\n",
    "    \n",
    "    train_candidate_idx = sample(unlabeled_set$idx, selected_ins, replace = FALSE, prob = NULL)\n",
    "    train_candidates = unlabeled_set[idx %in% train_candidate_idx]\n",
    "    \n",
    "    return(train_candidates)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_variable_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_variable_importance <- function(model) {\n",
    "    importances <- importance(model, type = 1, scale = FALSE)\n",
    "    selected.vars <- order(importances, decreasing = TRUE)\n",
    "    ranked_features = feature_names[selected.vars]\n",
    "    ordered.importances <- importances[selected.vars]\n",
    "    \n",
    "    return(ranked_features)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_elimination <- function(h, total_numof_eliminated_vars, ranked_features) {\n",
    "    numof_columns_left = length(ranked_features) - (total_numof_eliminated_vars + h)\n",
    "    columns_left = ranked_features[1:numof_columns_left]\n",
    "    \n",
    "    eliminated_columns = setdiff((length(ranked_features) - total_numof_eliminated_vars),numof_columns_left)\n",
    "    eliminated_columns = ranked_features[eliminated_columns]\n",
    "    \n",
    "    # update total_numof_eliminated_vars\n",
    "    total_numof_eliminated_vars = length(ranked_features) - length(columns_left)\n",
    "    \n",
    "    return(list(columns_left, total_numof_eliminated_vars, h, eliminated_columns))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabeled Data Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latin hyper cube sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_pool.name= paste0(data.path,\"unlabeled_pool\",\"_\",model.type,\"_\",unlabeled_ins,\".csv\")\n",
    "unlabeled_pool <- fread(unlabeled_pool.name)  \n",
    "\n",
    "data_candidates = copy(unlabeled_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_candidates_plot = ggplot(data = data_candidates[,.(density,`%-similar-wanted`)], aes(x = density, y = `%-similar-wanted`)) +\n",
    "                    geom_point() +\n",
    "                    ggtitle(paste0(\"unlabeled data for model_ \", model.type))\n",
    "data_candidates_plot\n",
    "ggsave(paste0(outputs.path,\"unlabeled_\", model.type,\".png\"))\n",
    "\n",
    "#pca_unlabeled_pool <- princomp(data_candidates[,-c(\"idx\")], cor = TRUE, scores = TRUE)\n",
    "#pca_unlabeled_pool_components <- get_pca_ind(pca_unlabeled_pool)\n",
    "#p_unlabeled_pool <- ggplot(data = data.table(pca_unlabeled_pool_components$coord[,1:2]), aes(x = Dim.1, y = Dim.2)) +\n",
    "#                    geom_point() +\n",
    "#                    ggtitle(paste0(\"unlabeled data for model_ \", model.type)) \n",
    "#p_unlabeled_pool\n",
    "#ggsave(paste0(outputs.path,\"unlabeled_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set.name= paste0(data.path,\"test_set\",\"_\",model.type,\"_\",test_ins,\".csv\")\n",
    "test_set <- fread(test_set.name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_plot = ggplot(data = test_set[,.(density,`%-similar-wanted`)], aes(x = density, y = `%-similar-wanted`)) +\n",
    "                    geom_point() +\n",
    "                    ggtitle(paste0(\"test data for model_ \", model.type))\n",
    "test_set_plot\n",
    "ggsave(paste0(outputs.path,\"test_set_\", model.type,\".png\"))\n",
    "\n",
    "#pca_test_set <- princomp(test_set, cor = TRUE, scores = TRUE)\n",
    "#pca_test_set_components <- get_pca_ind(pca_test_set)\n",
    "#p_test_set <- ggplot(data = data.table(pca_test_set_components$coord[,1:2]), aes(x = Dim.1, y = Dim.2)) +\n",
    "#                    geom_point() +\n",
    "#                    ggtitle(paste0(\"test data for model_ \", model.type)) \n",
    "#p_test_set\n",
    "#ggsave(paste0(outputs.path,\"test_set_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark : One-shot sampling, No feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_all = data.table()\n",
    "\n",
    "for( i in seed.oneshot){\n",
    "    \n",
    "    training_set.name= paste0(data.path,\"training_set\",\"_\",model.type,\"_\",train_ins_oneshot,\"_seed\",i,\".csv\")\n",
    "    training_set <- fread(training_set.name) \n",
    "    \n",
    "    assign(paste0(\"training_set_\",i),training_set)\n",
    "    \n",
    "    training_set_all = rbind(training_set_all,data.table(training_set, \"seed\" = i))\n",
    "    rm(training_set,training_set.name)    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_data = copy(training_set_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot_plot = ggplot(data = one_shot_data, aes(x = density, y = `%-similar-wanted`)) +\n",
    "                    geom_point(aes(colour = output)) +\n",
    "                    facet_wrap(~ seed) +\n",
    "                    labs(legend = \"output\") +\n",
    "                    ggtitle(paste0(\"one_shot_data for model_ \", model.type))\n",
    "one_shot_plot\n",
    "ggsave(paste0(outputs.path,\"one_shot_data_\", model.type,\".png\"))\n",
    "\n",
    "#pca_training_set <- princomp(training_set_all[,.SD, .SDcols = !c(\"output\",\"seed\")], cor = TRUE, scores = TRUE)\n",
    "#\n",
    "#pca_training_set_components <- get_pca_ind(pca_training_set)\n",
    "#pca_training_set_components <-cbind(pca_training_set_components$coord[,1:2],training_set_all[,.SD, .SDcols = c(\"output\",\"seed\")])\n",
    "#p_training_set <- ggplot(data = pca_training_set_components, aes(x = Dim.1, y = Dim.2)) +\n",
    "#             geom_point(aes(colour = output)) +\n",
    "#             facet_wrap(~ seed) +\n",
    "#             labs(legend = \"output\") +\n",
    "#             ggtitle(paste0(\"one_shot_data for model_ \", model.type)) \n",
    "#p_training_set\n",
    "#ggsave(paste0(outputs.path,\"one_shot_data_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_table_oneshot = data.table(iter = numeric(),seed = numeric(),rep = numeric(), mae= numeric(),rmse= numeric(), mape = numeric())\n",
    "predictedLabels_oneshot_all = data.table()\n",
    "obb_error_oneshot_all = data.table(obb_error = numeric(),seed = numeric(),rep = numeric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for( i in seed.oneshot){\n",
    "    \n",
    "    training_set = copy(one_shot_data[seed == i,.SD,.SDcols = -c(\"seed\")])\n",
    "    for(r in 1:10){\n",
    "   \n",
    "    #tuning = tune.randomForest(x = training_set[, -c(\"output\")], y = training_set$output, ntree = c(100,200,300,400), mtry = c(1, 2, 3), tunecontrol = tune.control(sampling = \"cross\", cross = length(training_set$output)))\n",
    "    #model_oneshot <- randomForest(x = training_set[, -c(\"output\")], y = training_set$output, importance = TRUE,ntree = as.numeric(tuning$best.parameters$ntree), mtry = as.numeric(tuning$best.parameters$mtry))\n",
    "    model_oneshot <- randomForest(x = training_set[, -c(\"output\")], y = training_set$output, importance = TRUE,ntree = ntree, mtry = mtry)\n",
    "    model_Sub.path = paste0(outputs.path,\"model_oneshot_seed_\",i,\"_rep_\",r,\".rds\")    \n",
    "    saveRDS(model_oneshot, model_Sub.path)\n",
    "    \n",
    "    obb_error_oneshot <- obb_error_func(model_oneshot)\n",
    "    obb_error_oneshot_all = rbind(obb_error_oneshot_all,data.table(obb_error_oneshot,\"seed\" = i, \"rep\" = r),use.names=FALSE)\n",
    "    \n",
    "    test_prediction_oneshot = get_test_predictions(model_oneshot,test_set,error_type)\n",
    "    predictedLabels_oneshot = test_prediction_oneshot[[1]]\n",
    "    \n",
    "    predictedLabels_oneshot_all = rbind(predictedLabels_oneshot_all,data.table(predictedLabels_oneshot, \"seed\" = i, \"rep\" = r))\n",
    "    performance_table_oneshot = rbind(performance_table_oneshot, data.table(1,i,r, test_prediction_oneshot[[2]]), use.names = FALSE)\n",
    "    output_variables = test_prediction_oneshot[[3]]   \n",
    "    \n",
    "    assign(paste0(\"model_oneshot_\",i,\"_rep_\",r),model_oneshot)\n",
    "    assign(paste0(\"obb_error_oneshot_\",i,\"_rep_\",r),obb_error_oneshot)\n",
    "    assign(paste0(\"test_prediction_oneshot_\",i,\"_rep_\",r),test_prediction_oneshot)\n",
    "    assign(paste0(\"predictedLabels_oneshot_\",i,\"_rep_\",r),predictedLabels_oneshot)\n",
    "    \n",
    "    rm(model_oneshot,obb_error_oneshot,test_prediction_oneshot,predictedLabels_oneshot)   \n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ggplot(data = performance_table_oneshot, aes(x = rep, y =rmse)) +\n",
    "                    geom_boxplot(aes(colour = rmse)) +\n",
    "                    facet_wrap(~ seed) +\n",
    "                    labs(legend = \"rmse\") \n",
    "a\n",
    "#ggsave(paste0(outputs.path,\"one_shot_data_\", model.type,\".png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ggplot(data = performance_table_oneshot, aes( y =rmse)) +\n",
    "                    geom_boxplot(aes(colour = rmse)) +\n",
    "                   # facet_wrap(~ seed) +\n",
    "                    labs(legend = \"rmse\") \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "head(performance_table_oneshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_molten_oneshot <- melt(data = performance_table_oneshot\n",
    "                             , id.vars = c('iter',\"seed\",\"rep\"))\n",
    "setnames(performance_molten_oneshot, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_oneshot_all = ggplot(performance_molten_oneshot, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "          geom_boxplot()+\n",
    "        #  facet_wrap(~seed) +\n",
    "        #  geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "          ggtitle(paste0(\"overall performances with one shot for model_ \", model.type))\n",
    "p_oneshot_all\n",
    "ggsave(paste0(outputs.path,\"performance_table_oneshot_all\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwrite(predictedLabels_oneshot_all, paste0(outputs.path,model.type,\"_\",\"predictedLabels_oneshot_all\",\".csv\"))\n",
    "fwrite(performance_table_oneshot, paste0(outputs.path,model.type,\"_\",\"performance_table_oneshot\",\".csv\"))\n",
    "fwrite(obb_error_oneshot_all, paste0(outputs.path,model.type,\"_\",\"obb_error_oneshot_all\",\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_oneshot <- ggplot(predictedLabels_oneshot_all,aes(x = get(output_variables[1]), y = get(output_variables[2]), color = (get(output_variables[2]) - get(output_variables[1])))) +\n",
    "            geom_point() +\n",
    "            geom_abline() +\n",
    "            facet_wrap(~ seed) + \n",
    "            xlab(\"actual values\") +\n",
    "            ylab(\"fitted values\") +\n",
    "            ggtitle(paste0(\"one_shot_predictions for model_ \", model.type)) \n",
    "\n",
    "p_oneshot\n",
    "ggsave(paste0(outputs.path,\"one_shot_predictions_\", model.type,\".png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling & No Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_Ad_all = data.table()\n",
    "for (i in seed.Ad) {\n",
    "    \n",
    "    training_set.name = paste0(data.path, \"training_set\", \"_\", model.type, \"_\", train_ins_Ad, \"_seed\", i, \".csv\")\n",
    "    training_set <- fread(training_set.name)\n",
    "    \n",
    "    assign(paste0(\"training_set_Ad_\", i), training_set)\n",
    "    \n",
    "    training_set_Ad_all = rbind(training_set_Ad_all, data.table(training_set, seed = i))\n",
    "    rm(training_set, training_set.name)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_initial_data = copy(training_set_Ad_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_initial_plot = ggplot(data = adaptive_initial_data, aes(x = density, y = `%-similar-wanted`)) +\n",
    "                           geom_point(aes(colour = output)) +\n",
    "                           facet_wrap(~seed) +\n",
    "                           labs(legend = \"output\") +\n",
    "                           ggtitle(paste0(\"initial_adaptive_data for model_ \", model.type))\n",
    "adaptive_initial_plot\n",
    "ggsave(paste0(outputs.path,\"adaptive_initial_data_\", model.type,\".png\"))\n",
    "\n",
    "#pca_training_set_Ad <- princomp(adaptive_initial_data[, .SD, .SDcols = !c(\"output\", \"seed\")], cor = TRUE, scores = TRUE)\n",
    "#\n",
    "#pca_training_set_Ad_components <- get_pca_ind(pca_training_set_Ad)\n",
    "#pca_training_set_Ad_components <- cbind(pca_training_set_Ad_components$coord[, 1:2],adaptive_initial_data[, .SD, .SDcols = c(\"output\", \"seed\")])\n",
    "#p_training_set_Ad <- ggplot(data = pca_training_set_Ad_components, aes(x = Dim.1,y = Dim.2)) + \n",
    "#                        geom_point(aes(colour = output)) + \n",
    "#                        facet_wrap(~seed) + \n",
    "#                        labs(legend = \"output\") +\n",
    "#                        ggtitle(paste0(\"initial_adaptive_data for model_ \", model.type)) \n",
    "#p_training_set_Ad\n",
    "#ggsave(paste0(outputs.path,\"adaptive_initial_data_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record model performances\n",
    "performance_table = data.table(iter = numeric(),seed = numeric(),rep = numeric(), mae = numeric(), rmse = numeric(), mape = numeric())\n",
    "# Record obb_error table\n",
    "obb_error = data.table(iter = numeric() ,obb_error = numeric(),seed = numeric(),rep = numeric())\n",
    "# Initialize record tables\n",
    "predictedLabels_all = data.table()\n",
    "train_candidates_all = data.table()\n",
    "training_set_Ad_final = data.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_candidates_all = data.table()\n",
    "for( i in seed.Ad){\n",
    "    \n",
    "    for(r in 1:10){\n",
    "        set.seed(i+r)\n",
    "     iter = 1   \n",
    "    unlabeled_pool =copy(data_candidates)  \n",
    "    predictedLabels_table = copy(test_set)\n",
    "    training_set_Ad = copy(adaptive_initial_data[seed == i,.SD,.SDcols = -c(\"seed\")])\n",
    "        \n",
    "    trainx = training_set_Ad[,.SD, .SDcols = feature_names]\n",
    "    trainy = training_set_Ad$output\n",
    "    \n",
    "    # Train the model\n",
    "    model_Sub <- randomForest( x = trainx, y =  trainy,importance = TRUE,ntree = ntree, mtry = mtry)\n",
    "    \n",
    "    # test the model on test set\n",
    "    test_predictions_Sub = get_test_predictions(model_Sub,test_set,error_type)\n",
    "    predictedLabels_Sub = test_predictions_Sub[[1]]\n",
    "    setnames(predictedLabels_Sub,c(\"pred_output\",error_type), c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter)))    \n",
    "    predictedLabels_table = cbind(predictedLabels_table,predictedLabels_Sub[,.SD, .SDcols = c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter))])\n",
    "    \n",
    "    # Keep test set error records\n",
    "    performance_table = rbind(performance_table,data.table(iter,i,r,test_predictions_Sub[[2]]), use.names = FALSE)    \n",
    "   \n",
    "    unlabeled_set <- copy(unlabeled_pool)\n",
    "    train_candidates = random_sample_selection(selected_ins,unlabeled_set)\n",
    "        \n",
    "    train_candidates_all = rbind(train_candidates_all, data.table(\"seed\" = i, \"rep\" = r, train_candidates))    \n",
    "    }\n",
    "}\n",
    "\n",
    "first_train_candidates = copy(train_candidates_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_train_candidates.Rd = ggplot(first_train_candidates, aes(x=idx, y = rep)) + \n",
    "                                geom_point( color = first_train_candidates$rep) + \n",
    "                                facet_wrap(~ seed)\n",
    "first_train_candidates.Rd\n",
    "ggsave(paste0(outputs.path,\"first_train_candidates_Rd_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record model performances\n",
    "performance_table = data.table(iter = numeric(),seed = numeric(),rep = numeric(), mae = numeric(), rmse = numeric(), mape = numeric())\n",
    "# Record obb_error table\n",
    "obb_error = data.table(iter = numeric() ,obb_error = numeric(),seed = numeric(),rep = numeric())\n",
    "# Initialize record tables\n",
    "predictedLabels_all = data.table()\n",
    "train_candidates_all = data.table()\n",
    "training_set_Ad_final = data.table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in seed.Ad) {\n",
    "    print(paste0(\"seed : \", i, \"  Random Sampling section start time : \", Sys.time()))\n",
    "    \n",
    "    for (r in 1:10) { #replications\n",
    "        set.seed(i + r)\n",
    "        print(paste0(\"seed : \", i,\"   rep : \", r, \"  Random Sampling section start time : \", Sys.time()))\n",
    "        unlabeled_pool = copy(data_candidates)\n",
    "        predictedLabels_table = copy(test_set)\n",
    "        training_set_Ad = copy(adaptive_initial_data[seed == i, .SD, .SDcols = -c(\"seed\")])\n",
    "        \n",
    "        train_candidates_table = data.table()\n",
    "            \n",
    "        iter = 1\n",
    "        while(iter <= iteration_budget){\n",
    "            print(iter)\n",
    "            \n",
    "            trainx = training_set_Ad[, .SD, .SDcols = feature_names]\n",
    "            trainy = training_set_Ad$output\n",
    "            \n",
    "            # Train the model\n",
    "            model_Sub <- randomForest(x = trainx, y = trainy, importance = TRUE, \n",
    "                ntree = ntree, mtry = mtry)\n",
    "            model_Sub.name = paste0(\"model_Rd_\", iter, \"_seed_\", i, \"_rep_\",r)\n",
    "            assign(model_Sub.name, model_Sub)\n",
    "            model_Sub.path = paste0(outputs.path, paste0(model_Sub.name, \".rds\"))\n",
    "            saveRDS(model_Sub, model_Sub.path)\n",
    "            \n",
    "            obb_error = rbind(obb_error, data.table(iter, obb_error_func(model_Sub), i, r), use.names = FALSE)\n",
    "            \n",
    "            # test the model on test set\n",
    "            test_predictions_Sub = get_test_predictions(model_Sub, test_set, error_type)\n",
    "            predictedLabels_Sub = test_predictions_Sub[[1]]\n",
    "            setnames(predictedLabels_Sub, c(\"pred_output\", error_type), c(paste0(\"pred_output_\", iter), paste0(error_type, \"_\", iter)))\n",
    "            \n",
    "            fwrite(predictedLabels_Sub\n",
    "                   ,paste0(outputs.path,model.type,\"_\",\"predictedLabels.Rd_seed_\",i,\"_iter_\",i,\"_rep_\",r,\".csv\") )\n",
    "            predictedLabels_table = cbind(predictedLabels_table, predictedLabels_Sub[,.SD, .SDcols = c(paste0(\"pred_output_\", iter), paste0(error_type, \"_\", iter))])\n",
    "            \n",
    "            # Keep test set error records\n",
    "            performance_table = rbind(performance_table, data.table(iter, i,r, test_predictions_Sub[[2]]),use.names = FALSE)\n",
    "            \n",
    "            if (iter != iteration_budget) {\n",
    "                # below efforts are unnecessary when the budget is reached.\n",
    "                \n",
    "                ## sample selection from unlabeled data select candidates\n",
    "                unlabeled_set <- copy(unlabeled_pool)\n",
    "                train_candidates = random_sample_selection(selected_ins, unlabeled_set)\n",
    "                \n",
    "                # Eliminate train candidates from the unlabeled pool\n",
    "                unlabeled_pool = unlabeled_pool[-train_candidates$idx]\n",
    "                rm(unlabeled_set)\n",
    "                \n",
    "                # run ABM to find outputs of train candidates\n",
    "                print(paste0(\"ABM train_candidate run start time : \", Sys.time()))\n",
    "                train_candidates = run_ABM(nofrep, selected_ins, train_candidates)\n",
    "                print(paste0(\"ABM train_candidate run end time : \", Sys.time()))\n",
    "                fwrite(train_candidates\n",
    "                       ,paste0(outputs.path,model.type,\"_\",\"train_candidates.Rd_seed_\",i,\"_iter_\",i,\"_rep_\",r,\".csv\") )\n",
    "                \n",
    "                train_candidates_table = rbind(train_candidates_table, data.table(train_candidates, iter = iter))\n",
    "                \n",
    "                # Add new data to train data\n",
    "                training_set_Ad = rbind(training_set_Ad, train_candidates[, -c(\"idx\")])\n",
    "            }\n",
    "            iter = iter + 1\n",
    "        }  \n",
    "        \n",
    "    assign(paste0(\"predictedLabels_table_\", i,\"_rep_\", r), predictedLabels_table)\n",
    "    training_set_Ad_final = rbind(training_set_Ad_final, data.table(training_set_Ad, seed = i,rep = r))  \n",
    "    assign(paste0(\"train_candidates_table_\", i,\"_rep_\", r), train_candidates_table)\n",
    "        \n",
    "    predictedLabels_all = rbind(predictedLabels_all, data.table(predictedLabels_table, seed = i, rep = r))\n",
    "    train_candidates_all = rbind(train_candidates_all, data.table(train_candidates_table, seed = i, rep = r))\n",
    "    \n",
    "    print(paste0(\"seed : \", i,\"   rep : \", r, \"  Random Sampling section end time : \", Sys.time()))        \n",
    "  }  \n",
    "         \n",
    "    print(paste0(\"seed : \", i, \"  Random Sampling section end time : \", Sys.time()))\n",
    "    #rm(training_set_Ad, predictedLabels_table, train_candidates_table)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_candidates_table_Rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final records\n",
    "FinalTrainData_Rd = copy(training_set_Ad_final)\n",
    "obb_error_Rd = copy(obb_error)\n",
    "performance_table_Rd = copy(performance_table)\n",
    "predictedLabels_table_Rd = copy(predictedLabels_all)\n",
    "train_candidates_table_Rd  = copy(train_candidates_all)\n",
    "\n",
    "# rm(training_set_Ad_final,obb_error,performance_table,predictedLabels_all,train_candidates_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwrite(FinalTrainData_Rd,paste0(outputs.path,model.type,\"_\",\"FinalTrainData_Rd\",\".csv\") )\n",
    "fwrite(performance_table_Rd,paste0(outputs.path,model.type,\"_\",\"performance_table_Rd\",\".csv\") )\n",
    "fwrite(train_candidates_table_Rd,paste0(outputs.path,model.type,\"_\",\"train_candidates_table_Rd\",\".csv\") )\n",
    "fwrite(predictedLabels_table_Rd,paste0(outputs.path,model.type,\"_\",\"predictedLabels_table_Rd\",\".csv\") )\n",
    "fwrite(obb_error_Rd,paste0(outputs.path,model.type,\"_\",\"obb_error_Rd\",\".csv\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show results\n",
    "nrow(FinalTrainData_Rd)\n",
    "performance_table_Rd \n",
    "train_candidates_table_Rd  \n",
    "head(predictedLabels_table_Rd)\n",
    "obb_error_Rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Replications in a Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_molten_Rd <- melt(data = performance_table_Rd\n",
    "                             , id.vars = c('iter',\"seed\",\"rep\"))\n",
    "setnames(performance_molten_Rd, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))\n",
    "p_Rd = ggplot(performance_molten_Rd, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "          geom_boxplot()+\n",
    "          facet_wrap(~seed) +\n",
    "        #  geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "          ggtitle(paste0(\"performances with Random Sampling for model_ \", model.type))\n",
    "p_Rd\n",
    "ggsave(paste0(outputs.path,\"performance_table_Rd_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall BoxPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_Rd_all = ggplot(performance_molten_Rd, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "          geom_boxplot()+\n",
    "        #  facet_wrap(~seed) +\n",
    "        #  geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "          ggtitle(paste0(\"overall performances with Random Sampling for model_ \", model.type))\n",
    "p_Rd_all\n",
    "ggsave(paste0(outputs.path,\"performance_table_Rd_all\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Rd_plot = ggplot(data = FinalTrainData_Rd, aes(x = density, y = `%-similar-wanted`)) +\n",
    "                     geom_point(aes(colour = output)) +\n",
    "                     facet_grid(rep ~ seed) +\n",
    "                     labs(legend = \"output\") +\n",
    "                     ggtitle(paste0(\"Final Data with Random Sampling for model_ \", model.type))\n",
    "final_Rd_plot\n",
    "ggsave(paste0(outputs.path,\"FinalTrainData_Rd_\", model.type,\".png\"))\n",
    "\n",
    "#pca_final_Rd_training_set <- princomp(FinalTrainData_Rd[,.SD, .SDcols = !c(\"output\",\"seed\")], cor = TRUE, scores = TRUE)\n",
    "#\n",
    "#pca_final_Rd_training_set_components <- get_pca_ind(pca_final_Rd_training_set)\n",
    "#pca_final_Rd_training_set_components <-cbind(pca_final_Rd_training_set_components$coord[,1:2],FinalTrainData_Rd[,.SD, .SDcols = c(\"output\",\"seed\")])\n",
    "#p_final_Rd_training_set <- ggplot(data = pca_final_Rd_training_set_components, aes(x = Dim.1, y = Dim.2)) +\n",
    "#             geom_point(aes(colour = output)) +\n",
    "#             facet_wrap(~ seed) +\n",
    "#             labs(legend = \"output\") +\n",
    "#             ggtitle(paste0(\"Final Data with Random Sampling for model_ \", model.type)) \n",
    "#p_final_Rd_training_set\n",
    "#ggsave(paste0(outputs.path,\"FinalTrainData_Rd_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#png(paste0(outputs.path,\"TrainData_Rd_Before_After_\", model.type,\".png\"))\n",
    "#grid.arrange(adaptive_initial_plot,final_Rd_plot, ncol=2)\n",
    "#dev.off()\n",
    "#grid.arrange(adaptive_initial_plot,final_Rd_plot, ncol=2)\n",
    "##ggsave(paste0(outputs.path,\"TrainData_Rd_Before_After_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Sampling & No Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize record tables Record train candidates\n",
    "train_candidates_table = data.table()\n",
    "\n",
    "# Record model performances\n",
    "performance_table = data.table(iter = numeric(),seed = numeric(),rep = numeric(), mae = numeric(), rmse = numeric(), mape = numeric())\n",
    "# Record obb_error table\n",
    "obb_error = data.table(iter = numeric() ,obb_error = numeric(),seed = numeric(),rep = numeric())\n",
    "\n",
    "## initialize variables\n",
    "# keep test set undistorted\n",
    "predictedLabels_table = copy(test_set)\n",
    "\n",
    "predictedLabels_all = data.table()\n",
    "train_candidates_all = data.table()\n",
    "training_set_Ad_final = data.table()\n",
    "importance_table_Ad = data.table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_budget\n",
    "selection_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.Ad = c(2,3,4,5,6,20)\n",
    "iteration_budget = 11\n",
    "selection_metric = \"sd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for( i in seed.Ad){\n",
    "#et.seed(10)\n",
    "print(paste0(\"seed : \",i,\"  Adaptive Sampling section start time : \",Sys.time()))\n",
    "    \n",
    "for (r in 1:10){ #replications\n",
    "    set.seed(r)\n",
    "    \n",
    "        print(paste0(\"seed : \", i,\"   rep : \", r, \"  Adaptive Sampling section start time : \", Sys.time()))\n",
    "        unlabeled_pool = copy(data_candidates)\n",
    "        predictedLabels_table = copy(test_set)\n",
    "        training_set_Ad = copy(adaptive_initial_data[seed == i, .SD, .SDcols = -c(\"seed\")])\n",
    "        \n",
    "        train_candidates_table = data.table()\n",
    "       \n",
    "iter = 1\n",
    "while(iter <= iteration_budget){   \n",
    "    print(iter)\n",
    "\n",
    "    trainx = training_set_Ad[,.SD, .SDcols = feature_names]\n",
    "    trainy = training_set_Ad$output\n",
    "    \n",
    "    # Train the model\n",
    "    model_Sub <- randomForest( x = trainx, y =  trainy,importance = TRUE,ntree = ntree, mtry = mtry)\n",
    "    model_Sub.name = paste0(\"model_Ad_\", iter, \"_seed_\", i, \"_rep_\",r)\n",
    "    assign(model_Sub.name,model_Sub)\n",
    "    model_Sub.path = paste0(outputs.path, paste0(model_Sub.name, \".rds\"))\n",
    "    saveRDS(model_Sub, model_Sub.path)\n",
    "                    \n",
    "    obb_error = rbind(obb_error,data.table(iter,obb_error_func(model_Sub),i,r),use.names=FALSE)\n",
    "\n",
    "    # test the model on test set\n",
    "    test_predictions_Sub = get_test_predictions(model_Sub,test_set,error_type)\n",
    "    predictedLabels_Sub = test_predictions_Sub[[1]]\n",
    "    setnames(predictedLabels_Sub,c(\"pred_output\",error_type), c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter)))    \n",
    "    fwrite(predictedLabels_Sub\n",
    "                   ,paste0(outputs.path,model.type,\"_\",\"predictedLabels.Ad_seed_\",i,\"_iter_\",i,\"_rep_\",r,\".csv\") )\n",
    "\n",
    "    predictedLabels_table = cbind(predictedLabels_table,predictedLabels_Sub[,.SD, .SDcols = c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter))])\n",
    "    \n",
    "    # Keep test set error records\n",
    "    performance_table = rbind(performance_table,data.table(iter,i,r,test_predictions_Sub[[2]]), use.names = FALSE)\n",
    "    \n",
    "    if(iter != iteration_budget){ # below efforts are unnecessary when the budget is reached.    \n",
    "    ## sample selection from unlabeled data select candidates\n",
    "        unlabeled_set <- copy(unlabeled_pool)\n",
    "        train_candidates = sample_selection(selected_ins, unlabeled_set, model_Sub)\n",
    "        \n",
    "        # eliminate candidates from the unlabeled pool\n",
    "        unlabeled_pool = unlabeled_pool[-train_candidates$idx]\n",
    "        rm(unlabeled_set)\n",
    "        \n",
    "        # run ABM to find outputs of train candidates\n",
    "        print(paste0(\"ABM train_candidate run start time : \",Sys.time()))\n",
    "        train_candidates = run_ABM(nofrep, selected_ins, train_candidates)\n",
    "        print(paste0(\"ABM train_candidate run end time : \",Sys.time()))\n",
    "        fwrite(train_candidates\n",
    "                       ,paste0(outputs.path,model.type,\"_\",\"train_candidates.Ad_seed_\",i,\"_iter_\",i,\"_rep_\",r,\".csv\") )\n",
    "        train_candidates_table = rbind(train_candidates_table, data.table(train_candidates,iter = iter))\n",
    "        \n",
    "        # add labeled candidates to the train data\n",
    "        training_set_Ad = rbind(training_set_Ad, train_candidates[, -c(\"idx\")])\n",
    "    }\n",
    "    \n",
    "    importance_table_Ad = rbind(importance_table_Ad, data.table(seed = i, rep = r, iter_no = iter, \n",
    "        t(importance(get(model_Sub.name), type = 1, scale = FALSE))))\n",
    "\n",
    "    iter = iter + 1\n",
    "}\n",
    "\n",
    "    \n",
    "assign(paste0(\"predictedLabels_table_\",i,\"_rep_\", r),predictedLabels_table)\n",
    "training_set_Ad_final = rbind(training_set_Ad_final, data.table(training_set_Ad, \"seed\" = i,rep = r))\n",
    "assign(paste0(\"train_candidates_table_\",i,\"_rep_\", r),train_candidates_table)\n",
    "\n",
    "predictedLabels_all = rbind(predictedLabels_all,data.table(predictedLabels_table,\"seed\" = i,rep = r))\n",
    "train_candidates_all = rbind(train_candidates_all,data.table(train_candidates_table,\"seed\" = i,rep = r))\n",
    "    \n",
    "print(paste0(\"seed : \",i,\"   rep : \", r,\"  Adaptive Sampling section end time : \",Sys.time()))\n",
    "}\n",
    "    \n",
    "print(paste0(\"seed : \",i,\"  Adaptive Sampling section end time : \",Sys.time()))\n",
    "#rm(training_set_Ad,predictedLabels_table,train_candidates_table)      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d =  t(predict(model_Ad_1_seed_0_rep_1, data_candidates, predict.all = TRUE)$individual) %>% \n",
    "#        data.table()\n",
    "#\n",
    "# d.range = sapply(d, range) %>% t() %>% data.table()\n",
    "#d.range = d.range[, .(range = abs(d.range[, 1] - d.range[, 2]))]\n",
    "#    setnames(d.range, \"range.V1\", \"range\")\n",
    "#  d.range[, `:=`(idx, 1:.N)]\n",
    "#f =  t(predict(model_Ad_1_seed_0_rep_2, data_candidates, predict.all = TRUE)$individual) %>% \n",
    "#        data.table()\n",
    "# f.range = sapply(f, range) %>% t() %>% data.table()\n",
    "#f.range = f.range[, .(range = abs(f.range[, 1] - f.range[, 2]))]\n",
    "#    setnames(f.range, \"range.V1\", \"range\")\n",
    "#f.range[, `:=`(idx, 1:.N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ggplot(d.range, aes(x = range)) + geom_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ggplot(f.range, aes(x = range)) + geom_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#d.range[idx %in% c(21,99,37,54,81,83)]\n",
    "#f.range[idx %in% c(21,99,37,54,81,83)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f.range[range >= 49.47747]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_candidates_all[rep == 1 & iter == 1]\n",
    "#train_candidates_all[rep == 2 & iter == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final records\n",
    "FinalTrainData_Ad = copy(training_set_Ad_final)\n",
    "obb_error_Ad = copy(obb_error)\n",
    "performance_table_Ad = copy(performance_table)\n",
    "predictedLabels_table_Ad = copy(predictedLabels_all)\n",
    "train_candidates_table_Ad  = copy(train_candidates_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(training_set_Ad_final,obb_error,performance_table,predictedLabels_all,train_candidates_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwrite(importance_table_Ad,paste0(outputs.path,model.type,\"_\",\"importance_table_Ad\",\".csv\") )\n",
    "fwrite(FinalTrainData_Ad,paste0(outputs.path,model.type,\"_\",\"FinalTrainData_Ad\",\".csv\") )\n",
    "fwrite(performance_table_Ad,paste0(outputs.path,model.type,\"_\",\"performance_table_Ad\",\".csv\") )\n",
    "fwrite(train_candidates_table_Ad,paste0(outputs.path,model.type,\"_\",\"train_candidates_table_Ad\",\".csv\") )\n",
    "fwrite(predictedLabels_table_Ad,paste0(outputs.path,model.type,\"_\",\"predictedLabels_table_Ad\",\".csv\") )\n",
    "fwrite(obb_error_Ad,paste0(outputs.path,model.type,\"_\",\"obb_error_Ad\",\".csv\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "head(importance_table_Ad)\n",
    "nrow(FinalTrainData_Ad)\n",
    "performance_table_Ad\n",
    "train_candidates_table_Ad\n",
    "head(predictedLabels_table_Ad)\n",
    "obb_error_Ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_train_candidates.Ad = ggplot(train_candidates_table_Ad[iter == 1], aes(x=idx, y = as.factor(rep))) + \n",
    "                                geom_point( color = train_candidates_table_Ad[iter == 1]$rep) + \n",
    "                                facet_wrap(~ seed)\n",
    "first_train_candidates.Ad\n",
    "ggsave(paste0(outputs.path,\"first_train_candidates_Ad_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#first_train_candidates.Ad = ggplot(train_candidates_table_Ad[iter == 1], aes(x=idx, y = rep)) + \n",
    "#                                geom_point( color = train_candidates_table_Ad[iter == 1]$rep) + \n",
    "#                                facet_wrap(~ seed)\n",
    "#first_train_candidates.Ad\n",
    "##ggsave(paste0(outputs.path,\"first_train_candidates_Ad_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Replications in a Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_molten_Ad <- melt(data = performance_table_Ad\n",
    "                             , id.vars = c('iter',\"seed\",\"rep\"))\n",
    "setnames(performance_molten_Ad, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))\n",
    "p_Ad = ggplot(performance_molten_Ad, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "          geom_boxplot()+\n",
    "          facet_wrap(~seed) +\n",
    "        #  geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "          ggtitle(paste0(\"performances with Adaptive Sampling for model_ \", model.type))\n",
    "p_Ad\n",
    "ggsave(paste0(outputs.path,\"performance_table_Ad_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_molten_Ad <- melt(data = performance_table_Ad\n",
    "                             , id.vars = c('iter',\"seed\",\"rep\"))\n",
    "setnames(performance_molten_Ad, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))\n",
    "p_Ad = ggplot(performance_molten_Ad, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "          geom_boxplot()+\n",
    "          facet_wrap(~seed) +\n",
    "        #  geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "          ggtitle(paste0(\"performances with Adaptive Sampling for model_ \", model.type))\n",
    "p_Ad\n",
    "ggsave(paste0(outputs.path,\"performance_table_Ad_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall BoxPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_Ad_all = ggplot(performance_molten_Ad, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "          geom_boxplot()+\n",
    "        #  facet_wrap(~seed) +\n",
    "        #  geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "          ggtitle(paste0(\"overall performances with Adaptive Sampling for model_ \", model.type))\n",
    "p_Ad_all\n",
    "ggsave(paste0(outputs.path,\"performance_table_Ad_all\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Ad_plot = ggplot(data = FinalTrainData_Ad, aes(x = density, y = `%-similar-wanted`)) +\n",
    "                     geom_point(aes(colour = output)) +\n",
    "                     facet_wrap(~ seed) +\n",
    "                     labs(legend = \"output\") +\n",
    "                     ggtitle(paste0(\"Final Data with Adaptive Sampling for model_ \", model.type))\n",
    "final_Ad_plot\n",
    "ggsave(paste0(outputs.path,\"FinalTrainData_Ad_\", model.type,\".png\"))\n",
    "\n",
    "#pca_final_Ad_training_set <- princomp(FinalTrainData_Ad[,.SD, .SDcols = !c(\"output\",\"seed\")], cor = TRUE, scores = TRUE)\n",
    "#\n",
    "#pca_final_Ad_training_set_components <- get_pca_ind(pca_final_Ad_training_set)\n",
    "#pca_final_Ad_training_set_components <-cbind(pca_final_Ad_training_set_components$coord[,1:2],FinalTrainData_Ad[,.SD, .SDcols = c(\"output\",\"seed\")])\n",
    "#p_final_Ad_training_set <- ggplot(data = pca_final_Ad_training_set_components, aes(x = Dim.1, y = Dim.2)) +\n",
    "#             geom_point(aes(colour = output)) +\n",
    "#             facet_wrap(~ seed) +\n",
    "#             labs(legend = \"output\") +\n",
    "#             ggtitle(paste0(\"Final Data with Adaptive Sampling for model_ \", model.type)) \n",
    "#p_final_Ad_training_set\n",
    "#ggsave(paste0(outputs.path,\"FinalTrainData_Ad_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png(paste0(outputs.path,\"TrainData_Ad_Before_After_\", model.type,\".png\"))\n",
    "grid.arrange(adaptive_initial_plot,final_Ad_plot, ncol=2)\n",
    "dev.off()\n",
    "grid.arrange(adaptive_initial_plot,final_Ad_plot, ncol=2)\n",
    "#ggsave(paste0(outputs.path,\"TrainData_Ad_Before_After_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png(paste0(outputs.path,\"3_cases\", model.type,\".png\"))\n",
    "grid.arrange(p_oneshot_all,p_Rd_all,p_Ad_all, ncol = 3)\n",
    "dev.off()\n",
    "grid.arrange(p_oneshot_all,p_Rd_all,p_Ad_all, ncol = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(FinalTrainData_Ad)\n",
    "nrow(one_shot_data)\n",
    "nrow(FinalTrainData_Rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_oneshot_table = data.table()\n",
    "pred_rd_table = data.table()\n",
    "pred_ad_table = data.table()\n",
    "\n",
    "for (i in seed.oneshot) {   \n",
    "    training_set = copy(one_shot_data[seed == i, .SD, .SDcols = -c(\"seed\")])\n",
    "    \n",
    "    oneshot_model = randomForest(x = training_set[, -c(\"output\")], y = training_set$output, \n",
    "        importance = TRUE, ntree = ntree, mtry = mtry)   \n",
    "    assign(paste0(\"oneshot_model_\", i), oneshot_model)\n",
    "    \n",
    "    pred_oneshot <- predict(oneshot_model, test_set)\n",
    "    pred_oneshot <- cbind(test_set, pred_oneshot)\n",
    "    pred_oneshot_table = rbind(pred_oneshot_table, data.table(pred_oneshot, seed = i))\n",
    "    \n",
    "    # pred_oneshot[, `:=`(RMSE, mapply(function(x, y) rmse_func(x, y), output, pred_oneshot))]\n",
    "    \n",
    "    assign(paste0(\"pred_oneshot_\", i), pred_oneshot)\n",
    "}\n",
    "\n",
    "\n",
    "for (i in seed.Ad) {   \n",
    "    training_set = copy(FinalTrainData_Rd[seed == i, .SD, .SDcols = -c(\"seed\")])\n",
    "    \n",
    "    random_model = randomForest(x = training_set[, .SD, .SDcols = feature_names], \n",
    "        y = training_set$output, importance = TRUE, ntree = ntree, mtry = mtry)   \n",
    "    assign(paste0(\"random_model_\", i), random_model)\n",
    "    \n",
    "    pred_rd <- predict(random_model, test_set)\n",
    "    pred_rd <- cbind(test_set, pred_rd)\n",
    "    pred_rd_table = rbind(pred_rd_table, data.table(pred_rd, seed = i))\n",
    "    \n",
    "    # pred_rd[, `:=`(RMSE, mapply(function(x, y) rmse_func(x, y), output, pred_rd))]\n",
    "    assign(paste0(\"pred_rd_\", i), pred_rd)\n",
    "    \n",
    "    \n",
    "    training_set = copy(FinalTrainData_Ad[seed == i, .SD, .SDcols = -c(\"seed\")])\n",
    "    \n",
    "    adaptive_model = randomForest(x = training_set[, .SD, .SDcols = feature_names], \n",
    "        y = training_set$output, importance = TRUE, ntree = ntree, mtry = mtry)\n",
    "    assign(paste0(\"adaptive_model_\", i), adaptive_model)\n",
    "    \n",
    "    pred_ad <- predict(adaptive_model, test_set)\n",
    "    pred_ad <- cbind(test_set, pred_ad)\n",
    "    pred_ad_table = rbind(pred_ad_table, data.table(pred_ad, seed = i))\n",
    "    \n",
    "    # pred_ad[, `:=`(RMSE, mapply(function(x, y) rmse_func(x, y), output, pred_ad))]\n",
    "    assign(paste0(\"pred_ad_\", i), pred_ad)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (i in seed.oneshot) {\n",
    "    print(paste0(\"seed: \",i))\n",
    "print(rmse(pred_oneshot_table[seed == i]$output,pred_oneshot_table[seed == i]$pred_oneshot))\n",
    "    }\n",
    "for (i in seed.Ad) {\n",
    "    print(paste0(\"seed: \",i))\n",
    "print(rmse(pred_rd_table[seed == i]$output,pred_rd_table[seed == i]$pred_rd))\n",
    "print(rmse(pred_ad_table[seed == i]$output,pred_ad_table[seed == i]$pred_ad))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwrite(pred_oneshot_table,paste0(outputs.path,model.type,\"_\",\"3scenario_oneshot\",\".csv\") )\n",
    "fwrite(pred_rd_table,paste0(outputs.path,model.type,\"_\",\"3scenario_rd\",\".csv\") )\n",
    "fwrite(pred_ad_table,paste0(outputs.path,model.type,\"_\",\"3scenario_ad\",\".csv\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling vs Uncertainty Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png(paste0(outputs.path,\"TrainData_Rd&Ad_Before_After_\", model.type,\".png\"))\n",
    "grid.arrange(one_shot_plot, adaptive_initial_plot,final_Rd_plot,final_Ad_plot,nrow=2, ncol=2)\n",
    "dev.off()\n",
    "grid.arrange(one_shot_plot, adaptive_initial_plot,final_Rd_plot,final_Ad_plot,nrow=2, ncol=2)\n",
    "\n",
    "#ggsave(paste0(outputs.path,\"TrainData_Rd&Ad_Before_After_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.arrange(p_Rd, p_Ad, ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_Rd_vs_Ad = rbind(performance_molten_Rd[,.(iter,seed,errortype,errorvalue, type = \"Rd\")],performance_molten_Ad[,.(iter,seed,errortype,errorvalue, type = \"Ad\")])\n",
    "p_Rd_vs_Ad = ggplot(performance_Rd_vs_Ad, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "            geom_line(lwd=1) +\n",
    "            geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "            facet_grid( seed ~ type ) +\n",
    "            ggtitle(\"performance_table_Rd&Ad\")\n",
    "p_Rd_vs_Ad\n",
    "ggsave(paste0(outputs.path,\"performance_table_Rd&Ad_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplotly(p_Rd_vs_Ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Sampling & Feature Elimination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on strategy:\n",
    "# Decide on strategy:\n",
    "if (model.type == \"basic\") {\n",
    "    sample_selection_iteration_order = c(1:(iteration_budget - 1))\n",
    "    feature_elimination_iteration_order = c(iteration_budget - 1)\n",
    "} else if (model.type == \"dummy\") {\n",
    "    sample_selection_iteration_order = c(1:(iteration_budget - 1))\n",
    "    feature_elimination_iteration_order = c((iteration_budget - 4):(iteration_budget - 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record model performances\n",
    "performance_table = data.table(iter = numeric(),seed = numeric(), mae = numeric(), rmse = numeric(), mape = numeric())\n",
    "\n",
    "# Record obb_error table\n",
    "obb_error = data.table(iter = numeric(), obb_error = numeric(),seed = numeric())\n",
    "\n",
    "# specify variables(columns) to be used initialize\n",
    "columns_left = feature_names\n",
    "total_numof_eliminated_vars <- 0\n",
    "\n",
    "predictedLabels_all = data.table()\n",
    "train_candidates_all = data.table()\n",
    "training_set_Ad_final = data.table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for( i in seed.Ad){\n",
    "set.seed(10)\n",
    "print(paste0(\"seed : \",i, \"   section start time : \",Sys.time()))\n",
    " \n",
    "    # keep sets undistorted\n",
    "unlabeled_pool =copy(data_candidates)    \n",
    "predictedLabels_table = copy(test_set)\n",
    "training_set_Ad = copy(adaptive_initial_data[seed == i,.SD,.SDcols = -c(\"seed\")])\n",
    "  \n",
    "train_candidates_table = data.table()\n",
    "iteration_history = data.table(iter_no = numeric(), IsFeatureEliminated = logical(), IsDataSelected = logical())\n",
    "    \n",
    "columns_left = feature_names\n",
    "total_numof_eliminated_vars <- 0\n",
    "    \n",
    "iter = 1\n",
    "while(iter <= iteration_budget){   \n",
    "    print(iter)\n",
    "\n",
    "    trainx = training_set_Ad[,.SD, .SDcols = columns_left]\n",
    "    trainy = training_set_Ad$output\n",
    "    \n",
    "    # Train the model\n",
    "    model_Sub <- randomForest( x = trainx, y =  trainy,importance = TRUE,ntree = ntree, mtry = mtry)\n",
    "    assign(paste0(\"model_AdFe_\",iter,\"_seed_\",i),model_Sub)\n",
    "    model_Sub.path = paste0(outputs.path,paste0(\"model_AdFe_\",iter,\"_seed_\",i),\".rds\")\n",
    "    saveRDS(model_Sub, model_Sub.path)\n",
    "    \n",
    "    if (length(columns_left) == length(feature_names)) {\n",
    "        ranked_features = get_variable_importance(model_Sub)\n",
    "    }\n",
    "    \n",
    "    obb_error = rbind(obb_error,data.table(iter,obb_error_func(model_Sub),i),use.names=FALSE)\n",
    "\n",
    "    # test the model on test set\n",
    "    test_predictions_Sub = get_test_predictions(model_Sub,test_set,error_type)\n",
    "    predictedLabels_Sub = test_predictions_Sub[[1]]\n",
    "    setnames(predictedLabels_Sub,c(\"pred_output\",error_type), c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter)))    \n",
    "    predictedLabels_table = cbind(predictedLabels_table,predictedLabels_Sub[,.SD, .SDcols = c(paste0(\"pred_output_\",iter),paste0(error_type,\"_\",iter))])\n",
    "    \n",
    "    # Keep test set error records\n",
    "    performance_table = rbind(performance_table,data.table(iter,i,test_predictions_Sub[[2]]), use.names = FALSE)\n",
    "    \n",
    "    # update iteration_history\n",
    "    iteration_history = rbind(iteration_history, data.table(iter, 0, 0), use.names = FALSE)\n",
    "    \n",
    "    \n",
    "    if(iter != iteration_budget){ # below efforts are unnecessary when the budget is reached.    \n",
    "        if (iter %in% sample_selection_iteration_order) {\n",
    "        ## sample selection from unlabeled data select candidates\n",
    "        unlabeled_set <- copy(unlabeled_pool)\n",
    "        train_candidates = sample_selection(selected_ins, unlabeled_set, model_Sub)\n",
    "        \n",
    "        # eliminate candidates from the unlabeled pool\n",
    "        unlabeled_pool = unlabeled_pool[-train_candidates$idx]\n",
    "        rm(unlabeled_set)\n",
    "        \n",
    "        # run ABM to find outputs of train candidates\n",
    "        print(paste0(\"ABM train_candidate run start time : \",Sys.time()))\n",
    "        train_candidates = run_ABM(nofrep, selected_ins, train_candidates)\n",
    "        print(paste0(\"ABM train_candidate run end time : \",Sys.time()))\n",
    "        \n",
    "        train_candidates_table = rbind(train_candidates_table, data.table(train_candidates,iter = iter))\n",
    "        \n",
    "        # add labeled candidates to the train data\n",
    "        training_set_Ad = rbind(training_set_Ad, train_candidates[, -c(\"idx\")])\n",
    "    \n",
    "        # update iteration_history\n",
    "         iteration_history[iter]$IsDataSelected= 1\n",
    "        }\n",
    "        if (iter %in% feature_elimination_iteration_order) {\n",
    "            ## feature elimination apply feature elimination\n",
    "            feature_elimination_result = feature_elimination(h, total_numof_eliminated_vars, ranked_features)\n",
    "            \n",
    "            columns_left = feature_elimination_result[[1]]  # \n",
    "            eliminated_columns = feature_elimination_result[[4]]  #   not necessary\n",
    "            total_numof_eliminated_vars = as.numeric(feature_elimination_result[2])\n",
    "            numof_eliminated_vars = as.numeric(feature_elimination_result[3])  #   not necessary \n",
    "            \n",
    "            # update iteration_history\n",
    "            iteration_history[iter]$IsFeatureEliminated= 1\n",
    "        }\n",
    "    }\n",
    "    iter = iter + 1\n",
    "}\n",
    "print(paste0(\"seed : \",i,\"   section end time : \",Sys.time()))\n",
    "\n",
    "assign(paste0(\"columns_left_\",i),columns_left)\n",
    "assign(paste0(\"predictedLabels_table_\",i),predictedLabels_table)\n",
    "assign(paste0(\"train_candidates_table_\",i),train_candidates_table)\n",
    "\n",
    "training_set_Ad_final = rbind(training_set_Ad_final, data.table(training_set_Ad, \"seed\" = i))\n",
    "predictedLabels_all = rbind(predictedLabels_all,data.table(predictedLabels_table,\"seed\" = i))\n",
    "train_candidates_all = rbind(train_candidates_all,data.table(train_candidates_table,\"seed\" = i))\n",
    "\n",
    "rm(training_set_Ad,predictedLabels_table,train_candidates_table)    \n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importance_table_AdFe = data.table()\n",
    "\n",
    "for (i in seed.Ad) {\n",
    "    iter = 1\n",
    "    while (iter <= (iteration_budget - 4)) {\n",
    "        importance_table_AdFe = rbind(importance_table_AdFe, data.table(seed = i, iter_no = iter, \n",
    "            t(importance(get(paste0(\"model_AdFe_\", iter, \"_seed_\", i)), type = 1, scale = FALSE))))\n",
    "        iter = iter + 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final records\n",
    "FinalTrainData_AdFe = copy(training_set_Ad_final)\n",
    "obb_error_AdFe = copy(obb_error)\n",
    "performance_table_AdFe = copy(performance_table)\n",
    "predictedLabels_table_AdFe = copy(predictedLabels_all)\n",
    "train_candidates_table_AdFe  = copy(train_candidates_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(training_set_Ad_final,obb_error,performance_table,predictedLabels_all,train_candidates_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwrite(importance_table_AdFe,paste0(outputs.path,model.type,\"_\",\"importance_table_AdFe\",\".csv\") )\n",
    "fwrite(FinalTrainData_AdFe,paste0(outputs.path,model.type,\"_\",\"FinalTrainData_AdFe\",\".csv\") )\n",
    "fwrite(performance_table_AdFe,paste0(outputs.path,model.type,\"_\",\"performance_table_AdFe\",\".csv\") )\n",
    "fwrite(train_candidates_table_AdFe,paste0(outputs.path,model.type,\"_\",\"train_candidates_table_AdFe\",\".csv\") )\n",
    "fwrite(predictedLabels_table_AdFe,paste0(outputs.path,model.type,\"_\",\"predictedLabels_table_AdFe\",\".csv\") )\n",
    "fwrite(obb_error_AdFe,paste0(outputs.path,model.type,\"_\",\"obb_error_AdFe\",\".csv\") )\n",
    "fwrite(iteration_history,paste0(outputs.path,model.type,\"_\",\"iteration_history\",\".csv\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrow(FinalTrainData_AdFe)\n",
    "performance_table_AdFe\n",
    "train_candidates_table_AdFe\n",
    "head(predictedLabels_table_AdFe)\n",
    "obb_error_AdFe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_molten_AdFe <- melt(data = performance_table_AdFe\n",
    "                             , id.vars = c('iter',\"seed\"))\n",
    "setnames(performance_molten_AdFe, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))\n",
    "p_AdFe = ggplot(performance_molten_AdFe, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "            geom_line(lwd=1) +\n",
    "            geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "            facet_wrap(~ seed) +\n",
    "            geom_vline(xintercept = iteration_history[IsFeatureEliminated==1]$iter_no + 1, linetype = \"dashed\") +\n",
    "            #geom_vline(xintercept = iteration_history[IsDataSelected==1]$iter_no + 1, linetype = \"dotdash\",color = \"yellow\") +\n",
    "            ggtitle(paste0(\"Performances with AdFe for model_ \", model.type))\n",
    "p_AdFe\n",
    "ggsave(paste0(outputs.path,\"performance_table_AdFe_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_AdFe_plot = ggplot(data = FinalTrainData_AdFe, aes(x = density, y = `%-similar-wanted`)) +\n",
    "                     geom_point(aes(colour = output)) +\n",
    "                     facet_wrap(~ seed) +\n",
    "                     labs(legend = \"output\") +\n",
    "                     ggtitle(paste0(\"Final Data with AdFe for model_ \", model.type))\n",
    "final_AdFe_plot\n",
    "ggsave(paste0(outputs.path,\"FinalTrainData_AdFe_\", model.type,\".png\"))\n",
    "#pca_final_AdFe_training_set <- princomp(FinalTrainData_AdFe[,.SD, .SDcols = !c(\"output\",\"seed\")], cor = TRUE, scores = TRUE)\n",
    "#\n",
    "#pca_final_AdFe_training_set_components <- get_pca_ind(pca_final_AdFe_training_set)\n",
    "#pca_final_AdFe_training_set_components <-cbind(pca_final_AdFe_training_set_components$coord[,1:2],FinalTrainData_AdFe[,.SD, .SDcols = c(\"output\",\"seed\")])\n",
    "#p_final_AdFe_training_set <- ggplot(data = pca_final_AdFe_training_set_components, aes(x = Dim.1, y = Dim.2)) +\n",
    "#             geom_point(aes(colour = output)) +\n",
    "#             facet_wrap(~ seed) +\n",
    "#             labs(legend = \"output\") +\n",
    "#             ggtitle(paste0(\"Final Data with AdFe for model_ \", model.type)) \n",
    "#p_final_AdFe_training_set\n",
    "#ggsave(paste0(outputs.path,\"FinalTrainData_AdFe_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Sampling with/without Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png(paste0(outputs.path,\"TrainData_Ad&AdFe_Before_After_\", model.type,\".png\"))\n",
    "grid.arrange(one_shot_plot, adaptive_initial_plot,final_Ad_plot,final_AdFe_plot,nrow=2, ncol=2)\n",
    "dev.off()\n",
    "grid.arrange(one_shot_plot, adaptive_initial_plot,final_Ad_plot,final_AdFe_plot,nrow=2, ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_Ad_vs_AdFe = rbind(performance_molten_Ad[,.(iter,seed,errortype,errorvalue, type = \"Ad\")], performance_molten_AdFe[,.(iter,seed,errortype,errorvalue, type = \"AdFe\")])\n",
    "p_Ad_vs_AdFe = ggplot(performance_Ad_vs_AdFe, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "            geom_line(lwd=1) +\n",
    "            geom_vline(xintercept = iteration_history[IsFeatureEliminated==1]$iter_no + 1, linetype = \"dashed\") +\n",
    "            geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "            facet_grid( seed ~ type)  +\n",
    "            ggtitle(paste0(\"Performances with Ad and AdFe for model_ \", model.type))\n",
    "p_Ad_vs_AdFe\n",
    "ggsave(paste0(outputs.path,\"performance_table_Ad&AdFe_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ggplotly(p_Ad_vs_AdFe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_Rd_vs_Ad_vs_AdFe = rbind(performance_Rd_vs_Ad,performance_molten_AdFe[,.(iter,seed,errortype,errorvalue, type = \"AdFe\")])\n",
    "\n",
    "\n",
    "p_Rd_vs_Ad_vs_AdFe = ggplot(performance_Rd_vs_Ad_vs_AdFe, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "            geom_line(lwd=1) +\n",
    "            geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "            facet_grid( seed ~ type ) +\n",
    "            ggtitle(\"performance_table_Rd&Ad&AdFe\")\n",
    "p_Rd_vs_Ad_vs_AdFe\n",
    "ggsave(paste0(outputs.path,\"performance_table_Rd&Ad&Fe_\", model.type,\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed.alt.test = c(1)\n",
    "#\n",
    "#performance_compare_seed = copy(performance_Ad_vs_AdFe)\n",
    "#\n",
    "#for(s in seed.alt.test){\n",
    "#    test_set.name = paste0(data.path,\"test_set\",\"_\",model.type,\"_\",test_ins,\"_seed\",s,\".csv\")\n",
    "#    test_set <- fread(test_set.name) \n",
    "#    assign(paste0(\"test_set.\",s),copy(test_set))\n",
    "#    \n",
    "#    pca_test_set_components <- princomp(test_set, cor = TRUE, scores = TRUE) %>% get_pca_ind()\n",
    "#    p_test_set <- ggplot(data = data.table(pca_test_set_components$coord[,1:2]), aes(x = Dim.1, y = Dim.2)) +\n",
    "#                    geom_point() +\n",
    "#                    ggtitle(paste0(\"test data for model_ \", model.type)) \n",
    "#    assign(paste0(\"p_test_set.\",s),copy(p_test_set))\n",
    "#    \n",
    "## Record model performances\n",
    "#performance_table = data.table(iter = numeric(),seed = numeric(), mae = numeric(), rmse = numeric(), mape = numeric())\n",
    "#predictedLabels_all = data.table()  \n",
    "#    \n",
    "#for( i in seed.Ad){\n",
    "#    \n",
    "#    # keep test set undistorted\n",
    "#    predictedLabels_table = copy(test_set)\n",
    "#    iter = 1\n",
    "#    while (iter <= iteration_budget) {\n",
    "#    \n",
    "#    model_Sub = copy(get(paste0(\"model_AdFe_\",iter,\"_seed_\",i)))\n",
    "#    \n",
    "#     # Test the model on test set\n",
    "#    test_predictions_Sub = get_test_predictions(model_Sub, test_set, error_type)\n",
    "#    predictedLabels_Sub = test_predictions_Sub[[1]]\n",
    "#    setnames(predictedLabels_Sub, c(\"pred_output\", error_type), c(paste0(\"pred_output_\", iter), paste0(error_type, \"_\", iter)))\n",
    "#    predictedLabels_table = cbind(predictedLabels_table, predictedLabels_Sub[,.SD, .SDcols = c(paste0(\"pred_output_\", iter), paste0(error_type, \"_\", iter))])\n",
    "#    \n",
    "#    # Keep test set error records\n",
    "#    performance_table = rbind(performance_table, data.table(iter,i ,test_predictions_Sub[[2]]), use.names = FALSE)\n",
    "#    iter = iter + 1  \n",
    "#    }\n",
    "#    predictedLabels_all = rbind(predictedLabels_all,data.table(predictedLabels_table,\"seed\" = i))\n",
    "#\n",
    "#}\n",
    "#\n",
    "#    assign(paste0(\"performance_table_AdFe.\",s),copy(performance_table))\n",
    "#    assign(paste0(\"predictedLabels_table_AdFe.\",s),copy(predictedLabels_all))\n",
    "#    #fwrite(predictedLabels_table_AdFe,paste0(outputs.path,model.type,\"_\",\"predictedLabels_table_AdFe\",\".csv\") )\n",
    "#    #fwrite(performance_table_AdFe,paste0(outputs.path,model.type,\"_\",\"performance_table_AdFe\",\".csv\") )\n",
    "#    \n",
    "#    performance_molten <- melt(data = performance_table, id.vars = c('iter',\"seed\"))\n",
    "#    setnames(performance_molten, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))\n",
    "#    assign(paste0(\"performance_molten_AdFe.\",s),copy(performance_molten))\n",
    "#    \n",
    "#    performance_compare_seed = rbind(performance_compare_seed\n",
    "#                               ,performance_molten[,.(iter,seed,errortype,errorvalue, type = paste0(\"AdFe.\",s))])\n",
    "# }\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_performance_compare_seed = ggplot(performance_compare_seed, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "#            geom_line(lwd=1) +\n",
    "#            geom_vline(xintercept = iteration_history[IsFeatureEliminated==1]$iter_no + 1, linetype = \"dashed\") +\n",
    "#          #  geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "#            facet_grid( seed ~ type)  +\n",
    "#            ggtitle(paste0(\"Performances with Ad and AdFe for model_ \", model.type))\n",
    "#p_performance_compare_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##seed.alt.test = c(1)\n",
    "#test_ins_alt= c(150,200,300)\n",
    "#\n",
    "#performance_compare_size = copy(performance_Ad_vs_AdFe)\n",
    "#\n",
    "#for(n in test_ins_alt){\n",
    "#    test_set.name = paste0(data.path,\"test_set\",\"_\",model.type,\"_\",n,\".csv\")\n",
    "#    test_set <- fread(test_set.name) \n",
    "#    assign(paste0(\"test_set.size.\",n),copy(test_set))\n",
    "#    \n",
    "#    pca_test_set_components <- princomp(test_set, cor = TRUE, scores = TRUE) %>% get_pca_ind()\n",
    "#    p_test_set <- ggplot(data = data.table(pca_test_set_components$coord[,1:2]), aes(x = Dim.1, y = Dim.2)) +\n",
    "#                    geom_point() +\n",
    "#                    ggtitle(paste0(\"test data for model_ \", model.type)) \n",
    "#    assign(paste0(\"p_test_set.size.\",n),copy(p_test_set))\n",
    "#    \n",
    "## Record model performances\n",
    "#performance_table = data.table(iter = numeric(),seed = numeric(), mae = numeric(), rmse = numeric(), mape = numeric())\n",
    "#predictedLabels_all = data.table()  \n",
    "#    \n",
    "#for( i in seed.Ad){\n",
    "#    \n",
    "#    # keep test set undistorted\n",
    "#    predictedLabels_table = copy(test_set)\n",
    "#    iter = 1\n",
    "#    while (iter <= iteration_budget) {\n",
    "#    \n",
    "#    model_Sub = copy(get(paste0(\"model_AdFe_\",iter,\"_seed_\",i)))\n",
    "#    \n",
    "#     # Test the model on test set\n",
    "#    test_predictions_Sub = get_test_predictions(model_Sub, test_set, error_type)\n",
    "#    predictedLabels_Sub = test_predictions_Sub[[1]]\n",
    "#    setnames(predictedLabels_Sub, c(\"pred_output\", error_type), c(paste0(\"pred_output_\", iter), paste0(error_type, \"_\", iter)))\n",
    "#    predictedLabels_table = cbind(predictedLabels_table, predictedLabels_Sub[,.SD, .SDcols = c(paste0(\"pred_output_\", iter), paste0(error_type, \"_\", iter))])\n",
    "#    \n",
    "#    # Keep test set error records\n",
    "#    performance_table = rbind(performance_table, data.table(iter,i ,test_predictions_Sub[[2]]), use.names = FALSE)\n",
    "#    iter = iter + 1  \n",
    "#    }\n",
    "#    predictedLabels_all = rbind(predictedLabels_all,data.table(predictedLabels_table,\"seed\" = i))\n",
    "#\n",
    "#}\n",
    "#\n",
    "#    assign(paste0(\"performance_table_AdFe.size.\",n),copy(performance_table))\n",
    "#    assign(paste0(\"predictedLabels_table_AdFe.size.\",n),copy(predictedLabels_all))\n",
    "#    #fwrite(predictedLabels_table_AdFe,paste0(outputs.path,model.type,\"_\",\"predictedLabels_table_AdFe\",\".csv\") )\n",
    "#    #fwrite(performance_table_AdFe,paste0(outputs.path,model.type,\"_\",\"performance_table_AdFe\",\".csv\") )\n",
    "#    \n",
    "#    performance_molten <- melt(data = performance_table, id.vars = c('iter',\"seed\"))\n",
    "#    setnames(performance_molten, c(\"variable\",\"value\"),c(\"errortype\",\"errorvalue\"))\n",
    "#    assign(paste0(\"performance_molten_AdFe.size.\",n),copy(performance_molten))\n",
    "#    \n",
    "#    performance_compare_size = rbind(performance_compare_size\n",
    "#                               ,performance_molten[,.(iter,seed,errortype,errorvalue, type = paste0(\"AdFe.size.\",n))])\n",
    "# }\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_performance_compare_size = ggplot(performance_compare_size, aes(x = iter, y = errorvalue, group=errortype, col=errortype)) + \n",
    "#            geom_line(lwd=1) +\n",
    "#            geom_vline(xintercept = iteration_history[IsFeatureEliminated==1]$iter_no + 1, linetype = \"dashed\") +\n",
    "#          #  geom_hline(data = performance_molten_oneshot, aes(yintercept = errorvalue, group=errortype, col=errortype),stat = \"hline\", linetype = \"dashed\") +\n",
    "#            facet_grid( seed ~ type)  +\n",
    "#            ggtitle(paste0(\"Performances with Ad and AdFe for model_ \", model.type))\n",
    "#p_performance_compare_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#head(train_candidates_table_AdFe)\n",
    "#head(train_candidates_table_Ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#varImpPlot(model_Ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quit NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLQuit(nl.obj = nl.model)\n",
    "#NLQuit(all=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 3.6",
   "language": "R",
   "name": "ir36"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "80.9983px",
    "width": "167.995px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320.174px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
